{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67255a8-4b93-4ecf-bc96-cf87cc58908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 21:03:10.170447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import pickle\n",
    "import dlib\n",
    "\n",
    "from deepface import DeepFace\n",
    "#from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace, DeepID\n",
    "from deepface.extendedmodels import Age, Gender, Race, Emotion\n",
    "#from deepface.modules import verification\n",
    "from deepface.commons import functions, distance as dst\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,Dropout,LSTM,\\\n",
    "                            TimeDistributed,Flatten,Dense,Bidirectional,ConvLSTM2D,MaxPooling3D,AveragePooling2D,Lambda,\\\n",
    "                            Activation,BatchNormalization\n",
    "\n",
    "from tensorflow import keras\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1223f05e-afc9-4e4a-a5d3-2182d665c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "rd.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce254fb5-e80f-4b0b-a812-fd76ce0cc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/var/scratch/mpa326/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3565466-aff6-4f8c-b73f-277c7d728887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Video Features/video_train_50m_50f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_50m_50f = df['labels']\n",
    "gender_train_50m_50f = df['genders']\n",
    "X_train_50m_50f=df['features']\n",
    "paths_train_50m_50f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71717c5-5f7b-4089-a501-f7b3dca89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Video Features/video_train_40m_60f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_40m_60f = df['labels']\n",
    "gender_train_40m_60f = df['genders']\n",
    "X_train_40m_60f=df['features']\n",
    "paths_train_40m_60f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06baeac-a54c-46c4-9313-29088ebf830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Video Features/video_train_60m_40f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_60m_40f = df['labels']\n",
    "gender_train_60m_40f = df['genders']\n",
    "X_train_60m_40f=df['features']\n",
    "paths_train_60m_40f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e22959-2908-4441-be65-9ddfa706a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Video Features/video_test_common','rb')\n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_test = df['labels']\n",
    "gender_test = df['genders']\n",
    "X_test=df['features']\n",
    "paths_test = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a71386-2a65-4229-a3f7-cd27339fa04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Audio Features/audio_train_50m_50f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_50m_50f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c04840f9-feb1-43bc-86d5-89b8b2efc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Audio Features/audio_train_40m_60f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_40m_60f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60fb5e99-3a53-4383-847a-a83af9eb156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Audio Features/audio_train_60m_40f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_60m_40f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b34245-47bf-47f3-8fc9-a7adb5cceb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/Ravdess_code/Audio Features/audio_test_common', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_test_aud=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94292a6e-5782-4b5c-a1fe-57c972743537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, random_state=27)\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, random_state=7)\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8c3b25-6908-46a0-9687-7d7a1e7694b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, random_state=27)\n",
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, random_state=7)\n",
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f3be07-b07e-4611-b031-2ea317fa5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, random_state=27)\n",
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, random_state=7)\n",
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d76b8fd7-6c44-4964-a045-412147fd5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_test_aud, y_test, gender_test = shuffle(X_test, X_test_aud, y_test, gender_test, random_state=27)\n",
    "X_test, X_test_aud, y_test, gender_test = shuffle(X_test, X_test_aud, y_test, gender_test, random_state=7)\n",
    "X_test, X_test_aud, y_test, gender_test = shuffle(X_test, X_test_aud, y_test, gender_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e130df04-4951-43ab-b7e2-a3026e782a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d\n",
      "max_pooling2d\n",
      "conv2d_1\n",
      "conv2d_2\n",
      "average_pooling2d\n",
      "conv2d_3\n",
      "conv2d_4\n",
      "average_pooling2d_1\n",
      "flatten\n",
      "dense\n",
      "dropout\n",
      "dense_1\n",
      "dropout_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "emotion=Emotion.loadModel()\n",
    "for layer in emotion.layers:\n",
    "    print(layer.name)\n",
    "features_emotion=Model(inputs=emotion.input, outputs=emotion.get_layer('dense_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6a4f1ef-1c43-4034-901e-ae226d474d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((960,20,48,48))\n",
    "for i in range(960):\n",
    "  for j in range(20):\n",
    "    img=X_train_50m_50f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_50m_50f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba106e9-b0fd-4689-b090-1c3324350584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((960,20,48,48))\n",
    "for i in range(960):\n",
    "  for j in range(20):\n",
    "    img=X_train_40m_60f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_40m_60f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31724a74-6afe-456b-bf5a-cca92ed15fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((960,20,48,48))\n",
    "for i in range(960):\n",
    "  for j in range(20):\n",
    "    img=X_train_60m_40f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_60m_40f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee80fe16-9297-431c-8106-933c98e16690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((288,20,48,48))\n",
    "for i in range(288):\n",
    "  for j in range(20):\n",
    "    img=X_test[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_test=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b991ca51-1750-49c6-aeea-86efb546a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_emotion.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50fd6932-da0b-40a1-bac3-20b875295dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Localization, self).__init__()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(20, [5, 5], activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(20, [5, 5], activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(20, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(6, activation=None, bias_initializer=tf.keras.initializers.constant([1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), kernel_initializer='zeros')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Localization Network with input shape:\", input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, 2, 3]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        theta = self.fc2(x)\n",
    "        theta = tf.keras.layers.Reshape((2, 3))(theta)\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8aac59e-01c6-4cd1-bd4c-ad43888290f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearInterpolation(tf.keras.layers.Layer):\n",
    "    def __init__(self, height=40, width=40):\n",
    "        super(BilinearInterpolation, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, self.height, self.width, 1]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'height': self.height,\n",
    "            'width': self.width,\n",
    "        }\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\n",
    "\n",
    "    def advance_indexing(self, inputs, x, y):\n",
    "        '''\n",
    "        Numpy like advance indexing is not supported in tensorflow, hence, this function is a hack around the same method\n",
    "        '''        \n",
    "        shape = tf.shape(inputs)\n",
    "        batch_size, _, _ = shape[0], shape[1], shape[2]\n",
    "        \n",
    "        batch_idx = tf.range(0, batch_size)\n",
    "        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, self.height, self.width))\n",
    "        indices = tf.stack([b, y, x], 3)\n",
    "        return tf.gather_nd(inputs, indices)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, theta = inputs\n",
    "        homogenous_coordinates = self.grid_generator(batch=tf.shape(images)[0])\n",
    "        return self.interpolate(images, homogenous_coordinates, theta)\n",
    "\n",
    "    def grid_generator(self, batch):\n",
    "        x = tf.linspace(-1, 1, self.width)\n",
    "        y = tf.linspace(-1, 1, self.height)\n",
    "            \n",
    "        xx, yy = tf.meshgrid(x, y)\n",
    "        xx = tf.reshape(xx, (-1,))\n",
    "        yy = tf.reshape(yy, (-1,))\n",
    "        homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\n",
    "        homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\n",
    "        homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\n",
    "        homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\n",
    "        return homogenous_coordinates\n",
    "    \n",
    "    def interpolate(self, images, homogenous_coordinates, theta):\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            transformed = tf.matmul(theta, homogenous_coordinates)\n",
    "            transformed = tf.transpose(transformed, perm=[0, 2, 1])\n",
    "            transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\n",
    "                \n",
    "            x_transformed = transformed[:, :, :, 0]\n",
    "            y_transformed = transformed[:, :, :, 1]\n",
    "                \n",
    "            x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\n",
    "            y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\n",
    "\n",
    "        with tf.name_scope(\"VariableCasting\"):\n",
    "            x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\n",
    "            x1 = x0 + 1\n",
    "            y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\n",
    "            y1 = y0 + 1\n",
    "\n",
    "            x0 = tf.clip_by_value(x0, 0, self.width-1)\n",
    "            x1 = tf.clip_by_value(x1, 0, self.width-1)\n",
    "            y0 = tf.clip_by_value(y0, 0, self.height-1)\n",
    "            y1 = tf.clip_by_value(y1, 0, self.height-1)\n",
    "            x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32)-1.0)\n",
    "            y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32)-1)\n",
    "\n",
    "        with tf.name_scope(\"AdvanceIndexing\"):\n",
    "            Ia = self.advance_indexing(images, x0, y0)\n",
    "            Ib = self.advance_indexing(images, x0, y1)\n",
    "            Ic = self.advance_indexing(images, x1, y0)\n",
    "            Id = self.advance_indexing(images, x1, y1)\n",
    "\n",
    "        with tf.name_scope(\"Interpolation\"):\n",
    "            x0 = tf.cast(x0, dtype=tf.float32)\n",
    "            x1 = tf.cast(x1, dtype=tf.float32)\n",
    "            y0 = tf.cast(y0, dtype=tf.float32)\n",
    "            y1 = tf.cast(y1, dtype=tf.float32)\n",
    "                            \n",
    "            wa = (x1-x) * (y1-y)\n",
    "            wb = (x1-x) * (y-y0)\n",
    "            wc = (x-x0) * (y1-y)\n",
    "            wd = (x-x0) * (y-y0)\n",
    "\n",
    "            wa = tf.expand_dims(wa, axis=3)\n",
    "            wb = tf.expand_dims(wb, axis=3)\n",
    "            wc = tf.expand_dims(wc, axis=3)\n",
    "            wd = tf.expand_dims(wd, axis=3)\n",
    "                        \n",
    "        return tf.math.add_n([wa*Ia + wb*Ib + wc*Ic + wd*Id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "043f386e-160c-475f-82ef-b0f0e6a7e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_en = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a140b7a1-fce9-4752-ab02-b14d403c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create__LRCN_with_STN() :\n",
    "  image = tf.keras.layers.Input(shape=(20,48,48,1))\n",
    "  theta = TimeDistributed(Localization())(image)\n",
    "  x = TimeDistributed(BilinearInterpolation(height=48, width=48))([image, theta])\n",
    "  features=TimeDistributed(features_emotion)(x)\n",
    "  lstm=Bidirectional(LSTM(200, activation='tanh',input_shape=(20, 1024),dropout=.3))(features)\n",
    "  out=Dense(len(emotions_en), activation = 'softmax')(lstm)\n",
    "\n",
    "  return tf.keras.models.Model(inputs=image, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e38dd31-8936-4e4e-acef-d80ddf63dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1e8bb91-d076-4944-9a90-7c8cdadb3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 7\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e499cdcf-9c0d-4172-8fdb-17ff31c7103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab0f32fd-3f59-46e1-8d92-952546668baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_alexnet():\n",
    "  AlexNet = Sequential()\n",
    "\n",
    "  #1st Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=96, input_shape=(128,128,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #2nd Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #3rd Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "\n",
    "  #4th Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "\n",
    "  #5th Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #Passing it to a Fully Connected layer\n",
    "  AlexNet.add(Flatten())\n",
    "  AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  # #2nd Fully Connected Layer\n",
    "  # AlexNet.add(Dense(4096))\n",
    "  # AlexNet.add(BatchNormalization())\n",
    "  # AlexNet.add(Activation('relu'))\n",
    "  # #Add Dropout\n",
    "  # AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  #3rd Fully Connected Layer\n",
    "  AlexNet.add(Dense(1000))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  #Add Dropout\n",
    "  AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  #Output Layer\n",
    "  AlexNet.add(Dense(8))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('softmax'))\n",
    "\n",
    "  return AlexNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "765473a1-c91b-4758-9379-ebb179803b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6548b499-5a1f-49e2-abd6-2bbef52f62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(201)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f6e31-1ed2-4f9b-a528-6af511a99bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness unimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c5150a0-a507-4658-bb47-441490f8d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 23s 2s/step - loss: 1.2369 - accuracy: 0.5694\n",
      "Test set evaluation: {'loss': 1.2369298934936523, 'accuracy': 0.5694444179534912}\n",
      "5/5 [==============================] - 7s 977ms/step\n",
      "5/5 [==============================] - 9s 2s/step\n",
      "female\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_50m50f_audio_model/'\n",
    "final_model = model_alexnet()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_1.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(X_test_aud, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = X_test_aud[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    print(gender)\n",
    "    predicted_label_1 = predicted_labels[gender_indices]\n",
    "    total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95c16acc-3927-417f-b902-db0b6abeab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/Ravdess_code/FairnessResults/fairness_analysis_ravdess_50m50f_audio_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5e47208-2166-4360-a475-2de2262487ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2369298934936523, 'accuracy': 0.5694444179534912}\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c7609fe-0813-4ca2-afa4-ccd26367c7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements: [0 1 2 3 4 5 6 7]\n",
      "Counts: [29 44 37 44 28 35 42 29]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(predicted_labels, return_counts=True)\n",
    "\n",
    "print(\"Unique elements:\", unique_elements)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05536496-1568-483a-9eb7-6c8e3c4a10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements: [0 1 2 3 4 5 6 7]\n",
      "Counts: [20 39 38 38 38 39 38 38]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Unique elements:\", unique_elements)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015ada4-7e46-49e7-8711-2142248facc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f98372c8-3315-4746-9ab8-ab6dd9918c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_indice = 3\n",
    "audio_indice = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9efbe91e-555a-4f65-87f2-6868cdca806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 21:22:11.284502: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4294967296 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_50m50f_audio_model/'\n",
    "model_audio = model_alexnet()\n",
    "model_audio.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_audio.load_weights(save_dir +\"model_\"+str(audio_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfd6dc9b-bdf3-4dba-b2ce-5be1205e3111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 48, 48, 1)\n",
      "Building Bilinear Interpolation Layer with input shape: ((None, 48, 48, 1), (None, 2, 3))\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.abspath(DIR)+'/ravdess_50m50f_video_model/'\n",
    "\n",
    "model_video = create__LRCN_with_STN()\n",
    "model_video.compile(loss='sparese_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_video.load_weights(save_dir+\"model_\"+str(video_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7af130f9-9401-4700-92a9-14b85d85541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_video_model=Model(inputs=model_video.input, outputs=model_video.get_layer('bidirectional_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca50913-0d86-4ad0-8506-6ed2ad4fc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_audio_model=Model(inputs=model_audio.input, outputs=model_audio.get_layer('dropout_9').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9a96364-2808-42ea-8ff3-85988cb1c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 7s 327ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video_test=features_video_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0668155e-5409-4f0a-92c0-2d36142d738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 19s 2s/step\n"
     ]
    }
   ],
   "source": [
    "features_audio_test=features_audio_model.predict(X_test_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe69204c-607c-47b8-9614-066dd13e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data_test = np.concatenate((features_video_test, features_audio_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b778c6c-c205-411e-b12c-f83e86f6fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 10s 340ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video=features_video_model.predict(X_train_50m_50f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a2c074c-25d9-4dec-908a-9e85b0d591b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 62s 2s/step\n"
     ]
    }
   ],
   "source": [
    "features_audio=features_audio_model.predict(X_train_aud_50m_50f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1049d8d-835d-463b-884d-b88bf819b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data_train=np.concatenate((features_video,features_audio),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fb348b3-d2b3-47e8-b66c-26aee7b96458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "fused_data_train = sc.fit_transform(fused_data_train)\n",
    "fused_data_test = sc.transform(fused_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5138e9a2-47c8-4300-b31d-2dfaba4a6721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 95% variance: 201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def find_n_components(data, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Determine the number of components to retain a specified amount of variance.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, shape (n_samples, n_features)\n",
    "      The data to perform PCA on.\n",
    "    - variance_threshold: float, optional (default=0.95)\n",
    "      The amount of variance that needs to be retained. Should be between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    - n_components: int\n",
    "      The number of components to retain the specified amount of variance.\n",
    "    \"\"\"\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Calculate cumulative variance\n",
    "    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    \n",
    "    # Find the number of components that satisfy the variance threshold\n",
    "    n_components = (cumulative_variance >= variance_threshold).argmax() + 1\n",
    "    \n",
    "    return n_components\n",
    "\n",
    "# Example usage:\n",
    "# Assuming fused_data_train is your training dataset\n",
    "n_components = find_n_components(fused_data_train, variance_threshold=0.95)\n",
    "print(f\"Number of components to retain 95% variance: {n_components}\")\n",
    "\n",
    "# Applying PCA with the determined number of components\n",
    "lda = PCA(n_components=n_components)\n",
    "fused_data_train = lda.fit_transform(fused_data_train)\n",
    "fused_data_test = lda.transform(fused_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e61505b-4e45-4901-8863-99823817b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(201)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "733c626e-f81c-4743-a8c0-be159967950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 0.7417 - accuracy: 0.7986\n",
      "Test set evaluation: {'loss': 0.741679310798645, 'accuracy': 0.7986111044883728}\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_50m50f_latefusion_PCA_n201_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_1.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(fused_data_test, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = fused_data_test[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "# for gender in np.unique(gender_test):\n",
    "#     gender_indices = np.where(gender_test == gender)\n",
    "#     print(gender)\n",
    "#     predicted_label_1 = predicted_labels[gender_indices]\n",
    "#     total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5f83ebc-aaab-4d7f-9485-5f2d9577453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/Ravdess_code/FairnessResults/fairness_analysis_ravdess_50m50f_latefusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391287d-ac4a-4e59-aaa3-bcc6ac3efbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness - Early Fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f87d2123-29ca-43aa-ab22-8f3740f15c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 49152)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened = np.reshape(X_train_40m_60f, (960, -1))\n",
    "\n",
    "X_train_aud_flattened = np.reshape(X_train_aud_40m_60f, (960, -1))\n",
    "X_train_aud_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab922791-074a-4fe1-9d3d-a7ec9d5394d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 49152)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_flattened = np.reshape(X_test, (288, -1))\n",
    "\n",
    "X_test_aud_flattened = np.reshape(X_test_aud, (288, -1))\n",
    "X_test_aud_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dfeecf51-bcac-4ae3-81e0-eaad6b7f5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "video_scaler = StandardScaler().fit(X_train_flattened)\n",
    "video_data_standardized = video_scaler.fit_transform(X_train_flattened)\n",
    "video_data_standardized_test = video_scaler.fit_transform(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b6f1ae1-60f1-4f27-8155-5bff0a8333af",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_scaler = StandardScaler().fit(X_train_aud_flattened)\n",
    "audio_data_standardized = audio_scaler.fit_transform(X_train_aud_flattened)\n",
    "audio_data_standardized_test = audio_scaler.fit_transform(X_test_aud_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ab314c5-119c-47a0-b98e-ec33f3f65b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_video = PCA(n_components=0.95).fit(video_data_standardized)\n",
    "video_data_pca = pca_video.fit_transform(video_data_standardized)\n",
    "video_data_pca_test = pca_video.transform(video_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7fd8b512-6041-47d5-918f-8ac0207fa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_audio = PCA(n_components=0.95).fit(audio_data_standardized)\n",
    "audio_data_pca = pca_audio.fit_transform(audio_data_standardized)\n",
    "audio_data_pca_test = pca_audio.transform(audio_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10d88597-5816-4a20-9e20-5cb182ba5bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 326)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e1dee2c-9066-4f64-bda4-1ecca602c271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 648)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "778a9eae-7061-493b-8321-5228d212aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_data = np.concatenate((video_data_pca_test, audio_data_pca_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0326079f-fb4c-4216-8f19-d2a721e8aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_data = np.concatenate((video_data_pca_test, audio_data_pca_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3604642-2b95-4414-a857-bfa96f783258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(974)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43a78dc1-f7d4-442f-ad2d-de61d3f15461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 15ms/step - loss: 1.0438 - accuracy: 0.6840\n",
      "Test set evaluation: {'loss': 1.0437825918197632, 'accuracy': 0.6840277910232544}\n",
      "5/5 [==============================] - 0s 20ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      "female\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_40m60f_earlyfusion_PCA_n0.95_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_5.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(concatenated_test_data, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = concatenated_test_data[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    print(gender)\n",
    "    predicted_label_1 = predicted_labels[gender_indices]\n",
    "    total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c370e3d8-c175-47e1-8613-b95c14cbdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/Ravdess_code/FairnessResults/fairness_analysis_ravdess_40m60f_earlyfusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182232ff-6025-42de-b3c7-dffb00b352f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness MidFusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e9bee0d9-4ab8-4f3e-abe7-8805c7c824b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "audio_indice = 1\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_40m60f_audio_model/'\n",
    "model_audio = model_alexnet()\n",
    "model_audio.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_audio.load_weights(save_dir +\"model_\"+str(audio_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fa282697-cfdf-45e3-afa0-763b1a3f8f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 48, 48, 1)\n",
      "Building Bilinear Interpolation Layer with input shape: ((None, 48, 48, 1), (None, 2, 3))\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.abspath(DIR)+'/ravdess_40m60f_video_model/'\n",
    "video_indice = 4\n",
    "model_video = create__LRCN_with_STN()\n",
    "model_video.compile(loss='sparese_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_video.load_weights(save_dir+\"model_\"+str(video_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b4a42c44-949e-4a44-b947-9d8f4bafe70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_video_model=Model(inputs=model_video.input, outputs=model_video.get_layer('time_distributed_38').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d19c98d4-6f6f-4a1b-9dad-febd8647f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 39s 859ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video=features_video_model.predict(X_train_40m_60f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a73c4c8b-2ccf-47a3-aa6f-3a67f0c0b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = np.reshape(features_video, (960, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "551e5cee-23fa-486d-9207-68d117ddd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 4s 431ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video_test = features_video_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "837bc5f5-5b28-47f5-bbca-5bd8dbc17456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flattened = np.reshape(features_video_test, (288, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "af26ac20-1fa0-4102-8ea3-749717f583f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_audio_model=Model(inputs=model_audio.input, outputs=model_audio.get_layer('flatten_23').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "61da4c57-f8a8-45e3-bd92-2714bf6533e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 22s 640ms/step\n"
     ]
    }
   ],
   "source": [
    "features_audio=features_audio_model.predict(X_train_aud_40m_60f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "73e4d2bf-7d33-452f-9dfb-95e1fde5b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 5s 600ms/step\n"
     ]
    }
   ],
   "source": [
    "features_audio_test = features_audio_model.predict(X_test_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1d31ea21-ef85-4379-9f74-5cd0f342649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "video_scaler = StandardScaler().fit(X_train_flattened)\n",
    "video_data_standardized = video_scaler.fit_transform(X_train_flattened)\n",
    "video_data_standardized_test = video_scaler.fit_transform(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eb550c23-c7c7-435d-884c-25f8f3066272",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_scaler = StandardScaler().fit(features_audio)\n",
    "audio_data_standardized = audio_scaler.fit_transform(features_audio)\n",
    "audio_data_standardized_test = audio_scaler.fit_transform(features_audio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7cfcf623-aebb-40e1-af95-0f45b21f6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_video = PCA(n_components=0.95).fit(video_data_standardized)\n",
    "video_data_pca = pca_video.fit_transform(video_data_standardized)\n",
    "video_data_pca_test = pca_video.transform(video_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e76bd5bf-1c0c-4b2b-8cd5-eec77cc2d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_audio = PCA(n_components=0.95).fit(audio_data_standardized)\n",
    "audio_data_pca = pca_audio.fit_transform(audio_data_standardized)\n",
    "audio_data_pca_test = pca_audio.transform(audio_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "59766ae4-8986-4fea-9b6c-e97075ea4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_train_data = np.concatenate((video_data_pca, audio_data_pca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e72bb5f6-515a-4030-b353-349c52992d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_data = np.concatenate((video_data_pca_test, audio_data_pca_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3bf00f7b-2fa3-4fde-b617-f9c61998ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(1197)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "62fbf2e4-a875-423e-9382-d4c8a6bde45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 14ms/step - loss: 0.7430 - accuracy: 0.7847\n",
      "Test set evaluation: {'loss': 0.7430149912834167, 'accuracy': 0.7847222089767456}\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "female\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/ravdess_40m60f_intermediatefusion_PCA_n0.95_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_1.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(concatenated_test_data, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = concatenated_test_data[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    print(gender)\n",
    "    predicted_label_1 = predicted_labels[gender_indices]\n",
    "    total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ba37e1c9-80e5-414f-a560-47c537c9c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/Ravdess_code/FairnessResults/fairness_analysis_ravdess_40m60f_intermediatefusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d3479-1bca-4891-b3d7-e0686d785439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
