{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f537bda-a308-41a7-9c0e-c9141ad3233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 22:08:24.551298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import pickle\n",
    "import dlib\n",
    "\n",
    "from deepface import DeepFace\n",
    "from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace, DeepID\n",
    "from deepface.extendedmodels import Age, Gender, Race, Emotion\n",
    "#from deepface.modules import verification\n",
    "from deepface.commons import functions, distance as dst\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,Dropout,LSTM,\\\n",
    "                            TimeDistributed,Flatten,Dense,Bidirectional,ConvLSTM2D,MaxPooling3D,AveragePooling2D,Lambda,\\\n",
    "                            Activation,BatchNormalization\n",
    "\n",
    "from tensorflow import keras\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ab9ea6-fb8e-4b9d-b725-f1c49f988bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "rd.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66eba89b-1eef-4747-9482-c0d7c445a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/var/scratch/mpa326/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a86856c-7593-4495-961d-b499c72bb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Video Features/video_train_50m_50f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_50m_50f = df['labels']\n",
    "gender_train_50m_50f = df['genders']\n",
    "X_train_50m_50f=df['features']\n",
    "race_train_50m_50f = df['races']\n",
    "ethnicity_train_50m_50f = df['ethnicity']\n",
    "paths_train_50m_50f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6277db4-76d2-4ed3-aba1-795a22d799ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Video Features/video_train_40m_60f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_40m_60f = df['labels']\n",
    "gender_train_40m_60f = df['genders']\n",
    "X_train_40m_60f=df['features']\n",
    "race_train_40m_60f = df['races']\n",
    "ethnicity_train_40m_60f = df['ethnicity']\n",
    "paths_train_40m_60f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c64ab5c-0962-4a4b-9aa8-b227dd67d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Video Features/video_train_60m_40f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_train_60m_40f = df['labels']\n",
    "gender_train_60m_40f = df['genders']\n",
    "X_train_60m_40f=df['features']\n",
    "race_train_60m_40f = df['races']\n",
    "ethnicity_train_60m_40f = df['ethnicity']\n",
    "paths_train_60m_40f = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2017423b-34a4-4ce9-a354-2bef696bc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Video Features/video_test_common', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "\n",
    "y_test = df['labels']\n",
    "gender_test = df['genders']\n",
    "X_test=df['features']\n",
    "race_test = df['races']\n",
    "ethnicity_test = df['ethnicity']\n",
    "paths_test = df['video_files_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c872969f-4e65-4571-a926-59bf00982f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Audio Features/audio_train_50m_50f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_50m_50f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fc0606-ca62-4425-bf27-1939ae346fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Audio Features/audio_train_40m_60f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_40m_60f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b725d55-4891-4c72-ba89-734f6e44455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Audio Features/audio_train_60m_40f', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_train_aud_60m_40f=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a24868c-c829-47ec-bfff-052d42d307ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('/var/scratch/mpa326/CREMA-D_code/Audio Features/audio_test_common', 'rb')     \n",
    "df = pickle.load(dbfile)\n",
    "X_test_aud=df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39dd199-ef85-4810-8de4-3df39d9f69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f, random_state=27)\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f, random_state=7)\n",
    "X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f = shuffle(X_train_50m_50f, X_train_aud_50m_50f, y_train_50m_50f, gender_train_50m_50f, race_train_50m_50f, ethnicity_train_50m_50f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8604dd72-bf8a-4592-bc6a-f7b3f1f59919",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f, random_state=27)\n",
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f, random_state=7)\n",
    "X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f = shuffle(X_train_40m_60f, X_train_aud_40m_60f, y_train_40m_60f, gender_train_40m_60f, race_train_40m_60f, ethnicity_train_40m_60f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe89b7b9-454a-439e-b3ad-1c755d451757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f, random_state=27)\n",
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f, random_state=7)\n",
    "X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f = shuffle(X_train_60m_40f, X_train_aud_60m_40f, y_train_60m_40f, gender_train_60m_40f, race_train_60m_40f, ethnicity_train_60m_40f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e841ad9-f415-4eed-af34-ed726d130891",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test = shuffle(X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test, random_state=27)\n",
    "X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test = shuffle(X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test, random_state=7)\n",
    "X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test = shuffle(X_test, X_test_aud, y_test, gender_test, race_test, ethnicity_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ccb8d4-d61a-4071-8f1b-981ee98fe592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d\n",
      "max_pooling2d\n",
      "conv2d_1\n",
      "conv2d_2\n",
      "average_pooling2d\n",
      "conv2d_3\n",
      "conv2d_4\n",
      "average_pooling2d_1\n",
      "flatten\n",
      "dense\n",
      "dropout\n",
      "dense_1\n",
      "dropout_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "emotion=Emotion.loadModel()\n",
    "for layer in emotion.layers:\n",
    "    print(layer.name)\n",
    "features_emotion=Model(inputs=emotion.input, outputs=emotion.get_layer('dense_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869dfe03-7153-4ec1-93a0-850635ac897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((4678,20,48,48))\n",
    "for i in range(4678):\n",
    "  for j in range(20):\n",
    "    img=X_train_50m_50f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_50m_50f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63a22cf2-8559-4827-929b-a20dc75f9635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((4678,20,48,48))\n",
    "for i in range(4678):\n",
    "  for j in range(20):\n",
    "    img=X_train_40m_60f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_40m_60f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04fda67d-d641-4a89-b374-36ac7ce650b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((4677,20,48,48))\n",
    "for i in range(4677):\n",
    "  for j in range(20):\n",
    "    img=X_train_60m_40f[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_train_60m_40f=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff54e6a-bc83-4e3c-9751-7d75e55b4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = np.ones((1487,20,48,48))\n",
    "for i in range(1487):\n",
    "  for j in range(20):\n",
    "    img=X_test[i][j]\n",
    "    img=img.astype(np.float32)\n",
    "    # df_[i][j]=cv2.resize(img,(48, 48))/255\n",
    "    df_[i][j]=img/255\n",
    "X_test=df_\n",
    "df_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cbe3efb-09d8-4589-89c0-bbfec4813ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_emotion.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b56f8a-23cd-4652-a48b-52f9a9fa5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Localization, self).__init__()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(20, [5, 5], activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(20, [5, 5], activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(20, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(6, activation=None, bias_initializer=tf.keras.initializers.constant([1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), kernel_initializer='zeros')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Localization Network with input shape:\", input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, 2, 3]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        theta = self.fc2(x)\n",
    "        theta = tf.keras.layers.Reshape((2, 3))(theta)\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69389e9f-1a34-47e3-b178-b38561434061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearInterpolation(tf.keras.layers.Layer):\n",
    "    def __init__(self, height=40, width=40):\n",
    "        super(BilinearInterpolation, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [None, self.height, self.width, 1]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'height': self.height,\n",
    "            'width': self.width,\n",
    "        }\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\n",
    "\n",
    "    def advance_indexing(self, inputs, x, y):\n",
    "        '''\n",
    "        Numpy like advance indexing is not supported in tensorflow, hence, this function is a hack around the same method\n",
    "        '''        \n",
    "        shape = tf.shape(inputs)\n",
    "        batch_size, _, _ = shape[0], shape[1], shape[2]\n",
    "        \n",
    "        batch_idx = tf.range(0, batch_size)\n",
    "        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, self.height, self.width))\n",
    "        indices = tf.stack([b, y, x], 3)\n",
    "        return tf.gather_nd(inputs, indices)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, theta = inputs\n",
    "        homogenous_coordinates = self.grid_generator(batch=tf.shape(images)[0])\n",
    "        return self.interpolate(images, homogenous_coordinates, theta)\n",
    "\n",
    "    def grid_generator(self, batch):\n",
    "        x = tf.linspace(-1, 1, self.width)\n",
    "        y = tf.linspace(-1, 1, self.height)\n",
    "            \n",
    "        xx, yy = tf.meshgrid(x, y)\n",
    "        xx = tf.reshape(xx, (-1,))\n",
    "        yy = tf.reshape(yy, (-1,))\n",
    "        homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\n",
    "        homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\n",
    "        homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\n",
    "        homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\n",
    "        return homogenous_coordinates\n",
    "    \n",
    "    def interpolate(self, images, homogenous_coordinates, theta):\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            transformed = tf.matmul(theta, homogenous_coordinates)\n",
    "            transformed = tf.transpose(transformed, perm=[0, 2, 1])\n",
    "            transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\n",
    "                \n",
    "            x_transformed = transformed[:, :, :, 0]\n",
    "            y_transformed = transformed[:, :, :, 1]\n",
    "                \n",
    "            x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\n",
    "            y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\n",
    "\n",
    "        with tf.name_scope(\"VariableCasting\"):\n",
    "            x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\n",
    "            x1 = x0 + 1\n",
    "            y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\n",
    "            y1 = y0 + 1\n",
    "\n",
    "            x0 = tf.clip_by_value(x0, 0, self.width-1)\n",
    "            x1 = tf.clip_by_value(x1, 0, self.width-1)\n",
    "            y0 = tf.clip_by_value(y0, 0, self.height-1)\n",
    "            y1 = tf.clip_by_value(y1, 0, self.height-1)\n",
    "            x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32)-1.0)\n",
    "            y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32)-1)\n",
    "\n",
    "        with tf.name_scope(\"AdvanceIndexing\"):\n",
    "            Ia = self.advance_indexing(images, x0, y0)\n",
    "            Ib = self.advance_indexing(images, x0, y1)\n",
    "            Ic = self.advance_indexing(images, x1, y0)\n",
    "            Id = self.advance_indexing(images, x1, y1)\n",
    "\n",
    "        with tf.name_scope(\"Interpolation\"):\n",
    "            x0 = tf.cast(x0, dtype=tf.float32)\n",
    "            x1 = tf.cast(x1, dtype=tf.float32)\n",
    "            y0 = tf.cast(y0, dtype=tf.float32)\n",
    "            y1 = tf.cast(y1, dtype=tf.float32)\n",
    "                            \n",
    "            wa = (x1-x) * (y1-y)\n",
    "            wb = (x1-x) * (y-y0)\n",
    "            wc = (x-x0) * (y1-y)\n",
    "            wd = (x-x0) * (y-y0)\n",
    "\n",
    "            wa = tf.expand_dims(wa, axis=3)\n",
    "            wb = tf.expand_dims(wb, axis=3)\n",
    "            wc = tf.expand_dims(wc, axis=3)\n",
    "            wd = tf.expand_dims(wd, axis=3)\n",
    "                        \n",
    "        return tf.math.add_n([wa*Ia + wb*Ib + wc*Ic + wd*Id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04361da1-b3c7-4a43-b1b4-30c59291d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_en = ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12910c77-02cc-4962-93f2-301b2d5de626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create__LRCN_with_STN() :\n",
    "  image = tf.keras.layers.Input(shape=(20,48,48,1))\n",
    "  theta = TimeDistributed(Localization())(image)\n",
    "  x = TimeDistributed(BilinearInterpolation(height=48, width=48))([image, theta])\n",
    "  features=TimeDistributed(features_emotion)(x)\n",
    "  lstm=Bidirectional(LSTM(200, activation='tanh',input_shape=(20, 1024),dropout=.3))(features)\n",
    "  out=Dense(len(emotions_en), activation = 'softmax')(lstm)\n",
    "\n",
    "  return tf.keras.models.Model(inputs=image, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26341311-23cd-4aa1-98db-af3d95c38d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71bc57c5-7fab-41e3-a4b0-704d59f2bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 7\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16604183-de4e-4d54-af3b-1ad861962fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Female appears 2338 times\n",
      "Value Male appears 2340 times\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(gender_train_50m_50f, return_counts=True)\n",
    "\n",
    "# Print the results\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value {value} appears {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f86dc24a-a277-4bc1-8074-66ad7c273b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value African American appears 1139 times\n",
      "Value Asian appears 346 times\n",
      "Value Caucasian appears 3140 times\n",
      "Value Unknown appears 53 times\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(race_train_50m_50f, return_counts=True)\n",
    "\n",
    "# Print the results\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value {value} appears {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d921319-9989-4edb-9660-298c9f9e34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Female appears 1870 times\n",
      "Value Male appears 2807 times\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(gender_train_60m_40f, return_counts=True)\n",
    "\n",
    "# Print the results\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value {value} appears {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d97bb21-a0dc-4179-933f-2eda53093637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value African American appears 345 times\n",
      "Value Asian appears 122 times\n",
      "Value Caucasian appears 1006 times\n",
      "Value Unknown appears 14 times\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(race_test, return_counts=True)\n",
    "\n",
    "# Print the results\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value {value} appears {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0f03ce0-def6-4b50-93d4-9c6bb457f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aud_50m_50f = X_train_aud_50m_50f/255\n",
    "X_train_aud_40m_60f = X_train_aud_40m_60f/255\n",
    "X_train_aud_60m_40f = X_train_aud_60m_40f/255\n",
    "X_test_aud = X_test_aud/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aabdfbc-59c2-45e5-a238-4ffc940dae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_alexnet():\n",
    "  AlexNet = Sequential()\n",
    "\n",
    "  #1st Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=96, input_shape=(128,128,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #2nd Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #3rd Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "\n",
    "  #4th Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "\n",
    "  #5th Convolutional Layer\n",
    "  AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))\n",
    "\n",
    "  #Passing it to a Fully Connected layer\n",
    "  AlexNet.add(Flatten())\n",
    "  AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  # #2nd Fully Connected Layer\n",
    "  # AlexNet.add(Dense(4096))\n",
    "  # AlexNet.add(BatchNormalization())\n",
    "  # AlexNet.add(Activation('relu'))\n",
    "  # #Add Dropout\n",
    "  # AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  #3rd Fully Connected Layer\n",
    "  AlexNet.add(Dense(1000))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('relu'))\n",
    "  #Add Dropout\n",
    "  AlexNet.add(Dropout(0.4))\n",
    "\n",
    "  #Output Layer\n",
    "  AlexNet.add(Dense(6))\n",
    "  AlexNet.add(BatchNormalization())\n",
    "  AlexNet.add(Activation('softmax'))\n",
    "\n",
    "  return AlexNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280a70a-2f7d-4e01-b7ac-973fe31dd67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LATE FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "505ab7e5-c643-475c-b247-d9b5dccb4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_indice = 3\n",
    "audio_indice = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf81c06e-0f60-40fa-b4d8-5ead9392ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_40m60f_audio_model/'\n",
    "model_audio = model_alexnet()\n",
    "model_audio.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_audio.load_weights(save_dir +\"model_\"+str(audio_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "554cb7df-4398-4e4d-94da-4cc046da223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 48, 48, 1)\n",
      "Building Bilinear Interpolation Layer with input shape: ((None, 48, 48, 1), (None, 2, 3))\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.abspath(DIR)+'/cremad_40m60f_video_model/'\n",
    "\n",
    "model_video = create__LRCN_with_STN()\n",
    "model_video.compile(loss='sparese_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_video.load_weights(save_dir+\"model_\"+str(video_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9474722e-fb2f-4354-844f-a18f8fb686dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_video_model=Model(inputs=model_video.input, outputs=model_video.get_layer('bidirectional').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f03d6c8-7b0c-4eae-80cb-6f0f82af77d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 28s 177ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video=features_video_model.predict(X_train_40m_60f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096b7953-aa1e-4a49-90fe-4f651d6bc559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 400)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0f956ad-c9b3-4eff-8899-dd384e8c5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_audio_model=Model(inputs=model_audio.input, outputs=model_audio.get_layer('dropout_3').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbbb480c-e587-463a-a992-2983c26ae63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 171s 1s/step\n"
     ]
    }
   ],
   "source": [
    "features_audio=features_audio_model.predict(X_train_aud_40m_60f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50bc69b4-a528-431a-a2e0-9bd129d9674b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c2cf320-652b-4823-8b27-d37b9408ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data_train=np.concatenate((features_video,features_audio),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2bfc2b4-6258-4393-b1ca-ce0dec69ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4678, 1400)\n",
      "[[ 0.27680057  0.40438     0.17499319 ...  0.8896747   0.33789122\n",
      "   0.        ]\n",
      " [-0.01256334 -0.731337   -0.11927307 ...  0.10878398  0.66160756\n",
      "   0.        ]\n",
      " [ 0.01654009  0.0363176  -0.19858484 ...  1.7530757   1.6893493\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.01857208 -0.8176755  -0.34945592 ...  0.          0.\n",
      "   0.41210353]\n",
      " [ 0.02107946  0.28272215  0.28229865 ...  1.0430069   0.37827185\n",
      "   0.        ]\n",
      " [ 0.00230955  0.0302401   0.06474552 ...  0.13205327  0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(fused_data_train.shape)\n",
    "print(fused_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42c91290-e6fa-4c86-a567-a47c1e9f77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 9s 182ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video_test=features_video_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e92ac47-cec0-44d5-8f94-c61c1db49935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 62s 1s/step\n"
     ]
    }
   ],
   "source": [
    "features_audio_test=features_audio_model.predict(X_test_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b7ddd7d-acc0-4c01-ab18-775779df551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data_test = np.concatenate((features_video_test, features_audio_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66716c5e-c480-4a5c-a333-549205d1a762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 1400)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7991ab35-4703-4a61-bf40-dcdb42b8895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "fused_data_train = sc.fit_transform(fused_data_train)\n",
    "fused_data_test = sc.transform(fused_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ca9f30b-f35f-4550-a83b-68f01c50bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 95% variance: 230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def find_n_components(data, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Determine the number of components to retain a specified amount of variance.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, shape (n_samples, n_features)\n",
    "      The data to perform PCA on.\n",
    "    - variance_threshold: float, optional (default=0.95)\n",
    "      The amount of variance that needs to be retained. Should be between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    - n_components: int\n",
    "      The number of components to retain the specified amount of variance.\n",
    "    \"\"\"\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Calculate cumulative variance\n",
    "    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    \n",
    "    # Find the number of components that satisfy the variance threshold\n",
    "    n_components = (cumulative_variance >= variance_threshold).argmax() + 1\n",
    "    \n",
    "    return n_components\n",
    "\n",
    "# Example usage:\n",
    "# Assuming fused_data_train is your training dataset\n",
    "n_components = find_n_components(fused_data_train, variance_threshold=0.95)\n",
    "print(f\"Number of components to retain 95% variance: {n_components}\")\n",
    "\n",
    "# Applying PCA with the determined number of components\n",
    "lda = PCA(n_components=n_components)\n",
    "fused_data_train = lda.fit_transform(fused_data_train)\n",
    "fused_data_test = lda.transform(fused_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0baddea-c326-4aad-8451-b76b08ebe97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 230)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38f29f1f-41a7-4458-b704-28d8881af351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(230)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(6))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93b58edc-a603-43ba-82ed-f9c03a63427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "936/936 [==============================] - 13s 12ms/step - loss: 0.9004 - accuracy: 0.7288 - val_loss: 0.9271 - val_accuracy: 0.6912\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.7579 - accuracy: 0.7803 - val_loss: 0.8818 - val_accuracy: 0.6955\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6897 - accuracy: 0.8044 - val_loss: 0.9134 - val_accuracy: 0.6795\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6534 - accuracy: 0.8129 - val_loss: 0.8884 - val_accuracy: 0.6955\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6049 - accuracy: 0.8239 - val_loss: 0.9019 - val_accuracy: 0.7105\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5592 - accuracy: 0.8391 - val_loss: 0.9120 - val_accuracy: 0.7115\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5383 - accuracy: 0.8498 - val_loss: 0.9160 - val_accuracy: 0.6944\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 12s 12ms/step - loss: 0.5259 - accuracy: 0.8568 - val_loss: 0.9363 - val_accuracy: 0.6955\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5171 - accuracy: 0.8640 - val_loss: 0.9087 - val_accuracy: 0.7030\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4758 - accuracy: 0.8688 - val_loss: 0.8873 - val_accuracy: 0.7051\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4559 - accuracy: 0.8704 - val_loss: 0.9505 - val_accuracy: 0.6934\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4319 - accuracy: 0.8781 - val_loss: 0.9559 - val_accuracy: 0.7041\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4434 - accuracy: 0.8811 - val_loss: 0.8930 - val_accuracy: 0.7126\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4055 - accuracy: 0.8960 - val_loss: 0.9800 - val_accuracy: 0.7062\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4270 - accuracy: 0.8886 - val_loss: 0.9789 - val_accuracy: 0.6880\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4260 - accuracy: 0.8872 - val_loss: 0.9358 - val_accuracy: 0.7073\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3896 - accuracy: 0.8998 - val_loss: 0.9616 - val_accuracy: 0.7062\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3988 - accuracy: 0.8819 - val_loss: 0.9195 - val_accuracy: 0.7073\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3791 - accuracy: 0.8960 - val_loss: 0.9365 - val_accuracy: 0.7019\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3530 - accuracy: 0.9102 - val_loss: 0.9516 - val_accuracy: 0.7009\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3673 - accuracy: 0.8998 - val_loss: 0.9758 - val_accuracy: 0.6976\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3628 - accuracy: 0.9043 - val_loss: 0.9654 - val_accuracy: 0.7009\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3419 - accuracy: 0.9017 - val_loss: 0.9267 - val_accuracy: 0.7009\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3410 - accuracy: 0.9118 - val_loss: 0.9486 - val_accuracy: 0.7083\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3491 - accuracy: 0.9075 - val_loss: 0.9762 - val_accuracy: 0.6987\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3344 - accuracy: 0.9129 - val_loss: 0.9708 - val_accuracy: 0.7019\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3445 - accuracy: 0.9086 - val_loss: 0.9144 - val_accuracy: 0.7126\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.3247 - accuracy: 0.9153 - val_loss: 0.9816 - val_accuracy: 0.6827\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.1025 - accuracy: 0.6604\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 13s 12ms/step - loss: 0.9909 - accuracy: 0.6750 - val_loss: 0.2251 - val_accuracy: 0.9776\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.8593 - accuracy: 0.7282 - val_loss: 0.1903 - val_accuracy: 0.9733\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.8131 - accuracy: 0.7338 - val_loss: 0.1660 - val_accuracy: 0.9605\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.7577 - accuracy: 0.7654 - val_loss: 0.1331 - val_accuracy: 0.9712\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7455 - accuracy: 0.7662 - val_loss: 0.1307 - val_accuracy: 0.9797\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6931 - accuracy: 0.7849 - val_loss: 0.1180 - val_accuracy: 0.9861\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6774 - accuracy: 0.7830 - val_loss: 0.1270 - val_accuracy: 0.9765\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6915 - accuracy: 0.7918 - val_loss: 0.1135 - val_accuracy: 0.9840\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6550 - accuracy: 0.7972 - val_loss: 0.1041 - val_accuracy: 0.9840\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6288 - accuracy: 0.8057 - val_loss: 0.1103 - val_accuracy: 0.9776\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6020 - accuracy: 0.8119 - val_loss: 0.1045 - val_accuracy: 0.9765\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6045 - accuracy: 0.8135 - val_loss: 0.1076 - val_accuracy: 0.9797\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6017 - accuracy: 0.8121 - val_loss: 0.1159 - val_accuracy: 0.9765\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5612 - accuracy: 0.8239 - val_loss: 0.1093 - val_accuracy: 0.9776\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5833 - accuracy: 0.8148 - val_loss: 0.1138 - val_accuracy: 0.9797\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5604 - accuracy: 0.8231 - val_loss: 0.1012 - val_accuracy: 0.9808\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5407 - accuracy: 0.8268 - val_loss: 0.1078 - val_accuracy: 0.9818\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5583 - accuracy: 0.8228 - val_loss: 0.1091 - val_accuracy: 0.9776\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5279 - accuracy: 0.8399 - val_loss: 0.1067 - val_accuracy: 0.9776\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5431 - accuracy: 0.8327 - val_loss: 0.1169 - val_accuracy: 0.9701\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5251 - accuracy: 0.8469 - val_loss: 0.1096 - val_accuracy: 0.9754\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.8123 - accuracy: 0.7216\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 12s 12ms/step - loss: 0.9962 - accuracy: 0.6740 - val_loss: 0.2784 - val_accuracy: 0.9551\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.8513 - accuracy: 0.7298 - val_loss: 0.2412 - val_accuracy: 0.9519\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 10s 11ms/step - loss: 0.7969 - accuracy: 0.7515 - val_loss: 0.2403 - val_accuracy: 0.9509\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 10s 11ms/step - loss: 0.7442 - accuracy: 0.7718 - val_loss: 0.2205 - val_accuracy: 0.9476\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7187 - accuracy: 0.7755 - val_loss: 0.2131 - val_accuracy: 0.9605\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6744 - accuracy: 0.7875 - val_loss: 0.2099 - val_accuracy: 0.9444\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6749 - accuracy: 0.7875 - val_loss: 0.1993 - val_accuracy: 0.9509\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6569 - accuracy: 0.7916 - val_loss: 0.2013 - val_accuracy: 0.9487\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6307 - accuracy: 0.8009 - val_loss: 0.1674 - val_accuracy: 0.9594\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6202 - accuracy: 0.8073 - val_loss: 0.1829 - val_accuracy: 0.9530\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5948 - accuracy: 0.8159 - val_loss: 0.1907 - val_accuracy: 0.9519\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6087 - accuracy: 0.8089 - val_loss: 0.2188 - val_accuracy: 0.9380\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5769 - accuracy: 0.8316 - val_loss: 0.2147 - val_accuracy: 0.9402\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5546 - accuracy: 0.8268 - val_loss: 0.2215 - val_accuracy: 0.9391\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5591 - accuracy: 0.8252 - val_loss: 0.2020 - val_accuracy: 0.9487\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5435 - accuracy: 0.8394 - val_loss: 0.1996 - val_accuracy: 0.9498\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5223 - accuracy: 0.8367 - val_loss: 0.1926 - val_accuracy: 0.9466\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5260 - accuracy: 0.8405 - val_loss: 0.2180 - val_accuracy: 0.9402\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5187 - accuracy: 0.8399 - val_loss: 0.1964 - val_accuracy: 0.9519\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5233 - accuracy: 0.8434 - val_loss: 0.2059 - val_accuracy: 0.9487\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.8264 - accuracy: 0.7209\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 12s 11ms/step - loss: 0.9928 - accuracy: 0.6783 - val_loss: 0.1869 - val_accuracy: 0.9711\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.8655 - accuracy: 0.7291 - val_loss: 0.1751 - val_accuracy: 0.9743\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.7959 - accuracy: 0.7547 - val_loss: 0.1367 - val_accuracy: 0.9829\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.7576 - accuracy: 0.7622 - val_loss: 0.1181 - val_accuracy: 0.9861\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7343 - accuracy: 0.7700 - val_loss: 0.1239 - val_accuracy: 0.9840\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7166 - accuracy: 0.7716 - val_loss: 0.1136 - val_accuracy: 0.9840\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6634 - accuracy: 0.7897 - val_loss: 0.1153 - val_accuracy: 0.9829\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6665 - accuracy: 0.7897 - val_loss: 0.1131 - val_accuracy: 0.9861\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6513 - accuracy: 0.7951 - val_loss: 0.1057 - val_accuracy: 0.9882\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.6293 - accuracy: 0.8042 - val_loss: 0.1034 - val_accuracy: 0.9840\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6198 - accuracy: 0.8007 - val_loss: 0.0983 - val_accuracy: 0.9850\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6084 - accuracy: 0.8074 - val_loss: 0.1132 - val_accuracy: 0.9818\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5888 - accuracy: 0.8173 - val_loss: 0.1061 - val_accuracy: 0.9861\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5891 - accuracy: 0.8173 - val_loss: 0.0983 - val_accuracy: 0.9807\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5911 - accuracy: 0.8239 - val_loss: 0.0883 - val_accuracy: 0.9861\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5700 - accuracy: 0.8197 - val_loss: 0.1112 - val_accuracy: 0.9775\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5325 - accuracy: 0.8394 - val_loss: 0.1069 - val_accuracy: 0.9786\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5492 - accuracy: 0.8328 - val_loss: 0.0978 - val_accuracy: 0.9882\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5720 - accuracy: 0.8207 - val_loss: 0.0966 - val_accuracy: 0.9818\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5446 - accuracy: 0.8346 - val_loss: 0.1023 - val_accuracy: 0.9829\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5451 - accuracy: 0.8360 - val_loss: 0.1093 - val_accuracy: 0.9850\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5421 - accuracy: 0.8303 - val_loss: 0.1064 - val_accuracy: 0.9840\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5376 - accuracy: 0.8378 - val_loss: 0.1019 - val_accuracy: 0.9861\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5124 - accuracy: 0.8472 - val_loss: 0.0953 - val_accuracy: 0.9829\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.8544 - accuracy: 0.7290\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 12s 12ms/step - loss: 0.9894 - accuracy: 0.6717 - val_loss: 0.2007 - val_accuracy: 0.9775\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.8631 - accuracy: 0.7280 - val_loss: 0.1888 - val_accuracy: 0.9786\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7980 - accuracy: 0.7579 - val_loss: 0.1454 - val_accuracy: 0.9797\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7606 - accuracy: 0.7678 - val_loss: 0.1420 - val_accuracy: 0.9818\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7402 - accuracy: 0.7628 - val_loss: 0.1334 - val_accuracy: 0.9850\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.7053 - accuracy: 0.7750 - val_loss: 0.1148 - val_accuracy: 0.9840\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6702 - accuracy: 0.7921 - val_loss: 0.1214 - val_accuracy: 0.9840\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6624 - accuracy: 0.7836 - val_loss: 0.1117 - val_accuracy: 0.9840\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6187 - accuracy: 0.8138 - val_loss: 0.1115 - val_accuracy: 0.9850\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6175 - accuracy: 0.8116 - val_loss: 0.1225 - val_accuracy: 0.9797\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.6254 - accuracy: 0.8050 - val_loss: 0.1100 - val_accuracy: 0.9861\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5938 - accuracy: 0.8186 - val_loss: 0.1042 - val_accuracy: 0.9840\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5886 - accuracy: 0.8186 - val_loss: 0.1018 - val_accuracy: 0.9893\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5932 - accuracy: 0.8146 - val_loss: 0.1084 - val_accuracy: 0.9797\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5875 - accuracy: 0.8226 - val_loss: 0.1042 - val_accuracy: 0.9829\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5635 - accuracy: 0.8247 - val_loss: 0.1146 - val_accuracy: 0.9775\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5497 - accuracy: 0.8378 - val_loss: 0.1105 - val_accuracy: 0.9786\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5387 - accuracy: 0.8346 - val_loss: 0.1049 - val_accuracy: 0.9786\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5368 - accuracy: 0.8354 - val_loss: 0.0980 - val_accuracy: 0.9807\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5308 - accuracy: 0.8378 - val_loss: 0.1082 - val_accuracy: 0.9797\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.5213 - accuracy: 0.8448 - val_loss: 0.0970 - val_accuracy: 0.9818\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.4861 - accuracy: 0.8523 - val_loss: 0.0993 - val_accuracy: 0.9818\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.5106 - accuracy: 0.8461 - val_loss: 0.1237 - val_accuracy: 0.9743\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4883 - accuracy: 0.8501 - val_loss: 0.0989 - val_accuracy: 0.9797\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4837 - accuracy: 0.8480 - val_loss: 0.1043 - val_accuracy: 0.9850\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4974 - accuracy: 0.8507 - val_loss: 0.1229 - val_accuracy: 0.9775\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 11s 12ms/step - loss: 0.4725 - accuracy: 0.8627 - val_loss: 0.1123 - val_accuracy: 0.9765\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 11s 11ms/step - loss: 0.4979 - accuracy: 0.8568 - val_loss: 0.1152 - val_accuracy: 0.9754\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.8560 - accuracy: 0.7209\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_ACCURACY_SPEECH = []\n",
    "VALIDATION_LOSS_SPEECH = []\n",
    "\n",
    "save_dir = os.path.abspath(DIR) + '/cremad_40m60f_latefusion_PCA_n230_model/'\n",
    "fold_var = 1\n",
    "\n",
    "for train_idx, val_idx in kfold.split(fused_data_train, y_train_40m_60f):\n",
    "    model=multi_modal_NN()\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_dir + get_model_name(fold_var), monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "# Start training the model.\n",
    "    LRCN_model_training_history = model.fit(      x = fused_data_train[train_idx],\n",
    "                                                  y = y_train_40m_60f[train_idx],\n",
    "                                                  validation_data=(fused_data_train[val_idx], y_train_40m_60f[val_idx]),\n",
    "                                                  epochs = 80,\n",
    "                                                  batch_size = 4,\n",
    "                                                  shuffle = True,\n",
    "                                                  callbacks = [checkpoint_cb, early_stopping_callback])\n",
    "    model.load_weights(save_dir + \"model_\" + str(fold_var) + \".h5\")\n",
    "\t\n",
    "    results = model.evaluate(fused_data_test, y_test)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "\t\n",
    "    VALIDATION_ACCURACY_SPEECH.append(results['accuracy'])\n",
    "    VALIDATION_LOSS_SPEECH.append(results['loss'])\n",
    "\t\n",
    "    tf.keras.backend.clear_session()\n",
    "\t\n",
    "    fold_var += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "380ec51a-f7e2-4f10-b19b-0db332f0de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 5ms/step - loss: 0.8574 - accuracy: 0.7256\n",
      "Test set evaluation: {'loss': 0.8574368357658386, 'accuracy': 0.7256220579147339}\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#Fairness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_40m60f_latefusion_PCA_n230_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_4.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(fused_data_test, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = fused_data_test[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a574beb3-51ba-4a88-8628-7b0f021e000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/CREMA-D_code/FairnessResults/fairness_analysis_cremad_40m60f_latefusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c73415-2b15-4377-82bc-ad8f3d955d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c163e705-7ab0-4160-bcd7-f71447f99aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 49152)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened = np.reshape(X_train_40m_60f, (4678, -1))\n",
    "\n",
    "X_train_aud_flattened = np.reshape(X_train_aud_40m_60f, (4678, -1))\n",
    "X_train_aud_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5dbd95e5-d729-418a-9f18-ed84ad94ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 49152)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_flattened = np.reshape(X_test, (1487, -1))\n",
    "\n",
    "X_test_aud_flattened = np.reshape(X_test_aud, (1487, -1))\n",
    "X_test_aud_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "170e7c88-f239-4e8a-a8ea-5dc465a01b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "video_scaler = StandardScaler().fit(X_train_flattened)\n",
    "video_data_standardized = video_scaler.fit_transform(X_train_flattened)\n",
    "video_data_standardized_test = video_scaler.fit_transform(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e2e1596-1629-419b-b403-cd8c5d5c5138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 46080)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data_standardized_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "579cf42a-058b-44ef-9ff5-28b7ff513a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_scaler = StandardScaler().fit(X_train_aud_flattened)\n",
    "audio_data_standardized = audio_scaler.fit_transform(X_train_aud_flattened)\n",
    "audio_data_standardized_test = audio_scaler.fit_transform(X_test_aud_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cdf320a-a97a-4591-9620-aeb09515c31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 49152)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_standardized_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9276a373-7dd1-47a3-bd50-64465b6ee8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_video = PCA(n_components=0.95).fit(video_data_standardized)\n",
    "video_data_pca = pca_video.fit_transform(video_data_standardized)\n",
    "video_data_pca_test = pca_video.transform(video_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9d3c90d-cd73-452e-b525-e71712ec459f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 1365)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af9b0310-ccab-4ea9-b325-6ac73d778010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_audio = PCA(n_components=0.95).fit(audio_data_standardized)\n",
    "audio_data_pca = pca_audio.fit_transform(audio_data_standardized)\n",
    "audio_data_pca_test = pca_audio.transform(audio_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "291318e9-f582-4bcb-9756-5ff60210a3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 720)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_pca_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30a24bb8-d035-4c95-8d65-d9a81f560232",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_train_data = np.concatenate((video_data_pca, audio_data_pca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9eee7bdc-9c44-4385-973d-fd2c9dba0955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 2085)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3ad6307-a454-4930-a74a-c5db255cc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_data = np.concatenate((video_data_pca_test, audio_data_pca_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f44cd2d3-cade-48ab-af5d-bb3221403bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 2085)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2d482ab-7fe9-4246-894a-5fbf957db15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(2085)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9eba11a8-fc25-4bfa-baf7-95000be8d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "936/936 [==============================] - 27s 28ms/step - loss: 1.7154 - accuracy: 0.3632 - val_loss: 1.2558 - val_accuracy: 0.5481\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.3874 - accuracy: 0.4757 - val_loss: 1.1045 - val_accuracy: 0.6175\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.2390 - accuracy: 0.5334 - val_loss: 1.0654 - val_accuracy: 0.6261\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.1346 - accuracy: 0.5820 - val_loss: 1.0379 - val_accuracy: 0.6293\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.0543 - accuracy: 0.6042 - val_loss: 1.0018 - val_accuracy: 0.6464\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9768 - accuracy: 0.6328 - val_loss: 0.9783 - val_accuracy: 0.6528\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.9252 - accuracy: 0.6638 - val_loss: 0.9801 - val_accuracy: 0.6282\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.8816 - accuracy: 0.6732 - val_loss: 0.9900 - val_accuracy: 0.6485\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8719 - accuracy: 0.6871 - val_loss: 0.9618 - val_accuracy: 0.6635\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8560 - accuracy: 0.6903 - val_loss: 1.0177 - val_accuracy: 0.6389\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7871 - accuracy: 0.7183 - val_loss: 0.9837 - val_accuracy: 0.6517\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7703 - accuracy: 0.7199 - val_loss: 1.0308 - val_accuracy: 0.6432\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7819 - accuracy: 0.7189 - val_loss: 1.0364 - val_accuracy: 0.6271\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7315 - accuracy: 0.7373 - val_loss: 1.0624 - val_accuracy: 0.6303\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7394 - accuracy: 0.7354 - val_loss: 1.0296 - val_accuracy: 0.6314\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7322 - accuracy: 0.7443 - val_loss: 1.0625 - val_accuracy: 0.6303\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6851 - accuracy: 0.7590 - val_loss: 1.0389 - val_accuracy: 0.6496\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6756 - accuracy: 0.7627 - val_loss: 1.0508 - val_accuracy: 0.6485\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6608 - accuracy: 0.7670 - val_loss: 1.0411 - val_accuracy: 0.6538\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6393 - accuracy: 0.7785 - val_loss: 1.0376 - val_accuracy: 0.6699\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6500 - accuracy: 0.7680 - val_loss: 1.0147 - val_accuracy: 0.6603\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6361 - accuracy: 0.7761 - val_loss: 1.0482 - val_accuracy: 0.6549\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6356 - accuracy: 0.7720 - val_loss: 1.0366 - val_accuracy: 0.6496\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6240 - accuracy: 0.7841 - val_loss: 1.0798 - val_accuracy: 0.6549\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6398 - accuracy: 0.7739 - val_loss: 1.1070 - val_accuracy: 0.6496\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6166 - accuracy: 0.7833 - val_loss: 1.1192 - val_accuracy: 0.6389\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5892 - accuracy: 0.7932 - val_loss: 1.1341 - val_accuracy: 0.6314\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6012 - accuracy: 0.7875 - val_loss: 1.1744 - val_accuracy: 0.6314\n",
      "Epoch 29/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.5788 - accuracy: 0.7982 - val_loss: 1.1194 - val_accuracy: 0.6346\n",
      "Epoch 30/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.5909 - accuracy: 0.7958 - val_loss: 1.1437 - val_accuracy: 0.6410\n",
      "Epoch 31/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.5645 - accuracy: 0.8022 - val_loss: 1.1871 - val_accuracy: 0.6303\n",
      "Epoch 32/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5692 - accuracy: 0.8036 - val_loss: 1.1578 - val_accuracy: 0.6143\n",
      "Epoch 33/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5565 - accuracy: 0.8100 - val_loss: 1.1790 - val_accuracy: 0.6250\n",
      "Epoch 34/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5565 - accuracy: 0.8025 - val_loss: 1.1634 - val_accuracy: 0.6635\n",
      "Epoch 35/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.5696 - accuracy: 0.8055 - val_loss: 1.1274 - val_accuracy: 0.6496\n",
      "47/47 [==============================] - 1s 8ms/step - loss: 1.0619 - accuracy: 0.6537\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 27s 27ms/step - loss: 1.7300 - accuracy: 0.3600 - val_loss: 1.2425 - val_accuracy: 0.5427\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.3786 - accuracy: 0.4824 - val_loss: 1.1452 - val_accuracy: 0.5684\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.2449 - accuracy: 0.5299 - val_loss: 1.1165 - val_accuracy: 0.5769\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.1166 - accuracy: 0.5812 - val_loss: 1.0393 - val_accuracy: 0.6100\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.0501 - accuracy: 0.6034 - val_loss: 1.0240 - val_accuracy: 0.6314\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9636 - accuracy: 0.6427 - val_loss: 1.0125 - val_accuracy: 0.6335\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9052 - accuracy: 0.6630 - val_loss: 1.0159 - val_accuracy: 0.6261\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9033 - accuracy: 0.6644 - val_loss: 1.0000 - val_accuracy: 0.6464\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8788 - accuracy: 0.6823 - val_loss: 1.0328 - val_accuracy: 0.6549\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.8316 - accuracy: 0.6954 - val_loss: 1.0544 - val_accuracy: 0.6325\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7782 - accuracy: 0.7127 - val_loss: 1.0716 - val_accuracy: 0.6346\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7757 - accuracy: 0.7186 - val_loss: 1.1310 - val_accuracy: 0.6132\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7639 - accuracy: 0.7274 - val_loss: 1.0916 - val_accuracy: 0.6207\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.7126 - accuracy: 0.7408 - val_loss: 1.1197 - val_accuracy: 0.6186\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7283 - accuracy: 0.7354 - val_loss: 1.1522 - val_accuracy: 0.6282\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7121 - accuracy: 0.7429 - val_loss: 1.1977 - val_accuracy: 0.6165\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6823 - accuracy: 0.7515 - val_loss: 1.1365 - val_accuracy: 0.6229\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6656 - accuracy: 0.7611 - val_loss: 1.2085 - val_accuracy: 0.6132\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.6622 - accuracy: 0.7622 - val_loss: 1.1755 - val_accuracy: 0.6100\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6580 - accuracy: 0.7670 - val_loss: 1.1548 - val_accuracy: 0.6026\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6520 - accuracy: 0.7712 - val_loss: 1.2145 - val_accuracy: 0.6218\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6248 - accuracy: 0.7771 - val_loss: 1.2074 - val_accuracy: 0.6154\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6146 - accuracy: 0.7910 - val_loss: 1.2089 - val_accuracy: 0.6250\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6167 - accuracy: 0.7777 - val_loss: 1.2275 - val_accuracy: 0.6261\n",
      "47/47 [==============================] - 1s 9ms/step - loss: 1.0459 - accuracy: 0.6469\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.7163 - accuracy: 0.3637 - val_loss: 1.2673 - val_accuracy: 0.5641\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 1.3613 - accuracy: 0.5005 - val_loss: 1.1073 - val_accuracy: 0.6015\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.2051 - accuracy: 0.5433 - val_loss: 0.9964 - val_accuracy: 0.6400\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.0832 - accuracy: 0.5877 - val_loss: 1.0465 - val_accuracy: 0.6090\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.0370 - accuracy: 0.6074 - val_loss: 0.9910 - val_accuracy: 0.6282\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.9425 - accuracy: 0.6521 - val_loss: 1.0244 - val_accuracy: 0.6207\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.9252 - accuracy: 0.6630 - val_loss: 1.0542 - val_accuracy: 0.6154\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.8822 - accuracy: 0.6815 - val_loss: 1.0928 - val_accuracy: 0.6047\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.8597 - accuracy: 0.6903 - val_loss: 1.1306 - val_accuracy: 0.6068\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.8141 - accuracy: 0.6983 - val_loss: 1.0677 - val_accuracy: 0.6197\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7845 - accuracy: 0.7173 - val_loss: 1.0695 - val_accuracy: 0.6303\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7647 - accuracy: 0.7266 - val_loss: 1.1249 - val_accuracy: 0.6282\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7607 - accuracy: 0.7298 - val_loss: 1.1178 - val_accuracy: 0.6197\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.7182 - accuracy: 0.7443 - val_loss: 1.1403 - val_accuracy: 0.6261\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.7135 - accuracy: 0.7402 - val_loss: 1.2044 - val_accuracy: 0.6026\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.7061 - accuracy: 0.7475 - val_loss: 1.1253 - val_accuracy: 0.6282\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.6634 - accuracy: 0.7598 - val_loss: 1.1141 - val_accuracy: 0.6271\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 25s 27ms/step - loss: 0.6729 - accuracy: 0.7606 - val_loss: 1.1288 - val_accuracy: 0.6335\n",
      "47/47 [==============================] - 1s 8ms/step - loss: 1.0093 - accuracy: 0.6389\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.7261 - accuracy: 0.3508 - val_loss: 1.2782 - val_accuracy: 0.5316\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.3385 - accuracy: 0.5017 - val_loss: 1.0723 - val_accuracy: 0.6096\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.1929 - accuracy: 0.5453 - val_loss: 1.0610 - val_accuracy: 0.6160\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.1023 - accuracy: 0.5797 - val_loss: 1.0516 - val_accuracy: 0.6310\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.0218 - accuracy: 0.6142 - val_loss: 1.0156 - val_accuracy: 0.6321\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9906 - accuracy: 0.6180 - val_loss: 1.0213 - val_accuracy: 0.6417\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9206 - accuracy: 0.6610 - val_loss: 1.0381 - val_accuracy: 0.6396\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8983 - accuracy: 0.6612 - val_loss: 1.0314 - val_accuracy: 0.6492\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8548 - accuracy: 0.6855 - val_loss: 1.0779 - val_accuracy: 0.6535\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8247 - accuracy: 0.6925 - val_loss: 1.0716 - val_accuracy: 0.6439\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8070 - accuracy: 0.6957 - val_loss: 1.0938 - val_accuracy: 0.6449\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7793 - accuracy: 0.7208 - val_loss: 1.0820 - val_accuracy: 0.6503\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7572 - accuracy: 0.7256 - val_loss: 1.1700 - val_accuracy: 0.6396\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7454 - accuracy: 0.7208 - val_loss: 1.1403 - val_accuracy: 0.6567\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7378 - accuracy: 0.7315 - val_loss: 1.1181 - val_accuracy: 0.6535\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7235 - accuracy: 0.7312 - val_loss: 1.1323 - val_accuracy: 0.6428\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6817 - accuracy: 0.7614 - val_loss: 1.0882 - val_accuracy: 0.6610\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6612 - accuracy: 0.7636 - val_loss: 1.1284 - val_accuracy: 0.6471\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6885 - accuracy: 0.7563 - val_loss: 1.1209 - val_accuracy: 0.6599\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6658 - accuracy: 0.7638 - val_loss: 1.1397 - val_accuracy: 0.6374\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6515 - accuracy: 0.7657 - val_loss: 1.1647 - val_accuracy: 0.6396\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6377 - accuracy: 0.7828 - val_loss: 1.1623 - val_accuracy: 0.6374\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6258 - accuracy: 0.7791 - val_loss: 1.2184 - val_accuracy: 0.6374\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6206 - accuracy: 0.7750 - val_loss: 1.1834 - val_accuracy: 0.6492\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6292 - accuracy: 0.7839 - val_loss: 1.2054 - val_accuracy: 0.6428\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6083 - accuracy: 0.7884 - val_loss: 1.1932 - val_accuracy: 0.6524\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5922 - accuracy: 0.7962 - val_loss: 1.2093 - val_accuracy: 0.6321\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5933 - accuracy: 0.7940 - val_loss: 1.1915 - val_accuracy: 0.6492\n",
      "Epoch 29/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.5787 - accuracy: 0.7999 - val_loss: 1.2213 - val_accuracy: 0.6342\n",
      "Epoch 30/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5703 - accuracy: 0.8034 - val_loss: 1.1592 - val_accuracy: 0.6578\n",
      "Epoch 31/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5831 - accuracy: 0.7986 - val_loss: 1.2170 - val_accuracy: 0.6396\n",
      "Epoch 32/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.5728 - accuracy: 0.8074 - val_loss: 1.2657 - val_accuracy: 0.6278\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 1.0468 - accuracy: 0.6503\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.7520 - accuracy: 0.3521 - val_loss: 1.2488 - val_accuracy: 0.5444\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.3815 - accuracy: 0.4761 - val_loss: 1.0663 - val_accuracy: 0.6160\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 1.2066 - accuracy: 0.5439 - val_loss: 1.0266 - val_accuracy: 0.6075\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.1080 - accuracy: 0.5701 - val_loss: 1.0144 - val_accuracy: 0.6385\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 1.0519 - accuracy: 0.6017 - val_loss: 0.9717 - val_accuracy: 0.6492\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.9952 - accuracy: 0.6270 - val_loss: 1.0002 - val_accuracy: 0.6332\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9147 - accuracy: 0.6522 - val_loss: 0.9883 - val_accuracy: 0.6406\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.9063 - accuracy: 0.6594 - val_loss: 0.9724 - val_accuracy: 0.6513\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8609 - accuracy: 0.6791 - val_loss: 1.0087 - val_accuracy: 0.6471\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.8141 - accuracy: 0.7147 - val_loss: 1.0143 - val_accuracy: 0.6374\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7973 - accuracy: 0.7123 - val_loss: 1.0694 - val_accuracy: 0.6182\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7937 - accuracy: 0.7147 - val_loss: 1.0601 - val_accuracy: 0.6310\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7649 - accuracy: 0.7318 - val_loss: 1.0168 - val_accuracy: 0.6396\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7659 - accuracy: 0.7219 - val_loss: 1.0772 - val_accuracy: 0.6225\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7315 - accuracy: 0.7451 - val_loss: 1.1017 - val_accuracy: 0.6289\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.7165 - accuracy: 0.7449 - val_loss: 1.0774 - val_accuracy: 0.6353\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.7072 - accuracy: 0.7425 - val_loss: 1.1152 - val_accuracy: 0.6385\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6778 - accuracy: 0.7545 - val_loss: 1.0977 - val_accuracy: 0.6182\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6684 - accuracy: 0.7716 - val_loss: 1.1297 - val_accuracy: 0.6385\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6581 - accuracy: 0.7681 - val_loss: 1.1219 - val_accuracy: 0.6246\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 26s 27ms/step - loss: 0.6679 - accuracy: 0.7660 - val_loss: 1.1798 - val_accuracy: 0.6310\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6343 - accuracy: 0.7793 - val_loss: 1.1270 - val_accuracy: 0.6310\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 26s 28ms/step - loss: 0.6360 - accuracy: 0.7702 - val_loss: 1.1267 - val_accuracy: 0.6428\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.9973 - accuracy: 0.6476\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_ACCURACY_SPEECH = []\n",
    "VALIDATION_LOSS_SPEECH = []\n",
    "\n",
    "save_dir = os.path.abspath(DIR) + '/cremad_40m60f_earlyfusion_PCA_n0.95_model/'\n",
    "fold_var = 1\n",
    "\n",
    "for train_idx, val_idx in kfold.split(concatenated_train_data, y_train_40m_60f):\n",
    "    model=multi_modal_NN()\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_dir + get_model_name(fold_var), monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "# Start training the model.\n",
    "    LRCN_model_training_history = model.fit(      x = concatenated_train_data[train_idx],\n",
    "                                                  y = y_train_40m_60f[train_idx],\n",
    "                                                  validation_data=(concatenated_train_data[val_idx], y_train_40m_60f[val_idx]),\n",
    "                                                  epochs = 80,\n",
    "                                                  batch_size = 4,\n",
    "                                                  shuffle = True,\n",
    "                                                  callbacks = [checkpoint_cb, early_stopping_callback])\n",
    "    model.load_weights(save_dir + \"model_\" + str(fold_var) + \".h5\")\n",
    "\t\n",
    "    results = model.evaluate(concatenated_test_data, y_test)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "\t\n",
    "    VALIDATION_ACCURACY_SPEECH.append(results['accuracy'])\n",
    "    VALIDATION_LOSS_SPEECH.append(results['loss'])\n",
    "\t\n",
    "    tf.keras.backend.clear_session()\n",
    "\t\n",
    "    fold_var += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1ce3491-54b7-401c-8bd4-6a6a1d969da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 7ms/step - loss: 1.0619 - accuracy: 0.6537\n",
      "Test set evaluation: {'loss': 1.061920166015625, 'accuracy': 0.6536651253700256}\n",
      "22/22 [==============================] - 0s 7ms/step\n",
      "25/25 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "#Fairness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_40m60f_earlyfusion_PCA_n0.95_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_1.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(concatenated_test_data, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = concatenated_test_data[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63635a5a-7888-4f62-a32c-1775b6037b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/CREMA-D_code/FairnessResults/fairness_analysis_cremad_40m60f_earlyfusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3e2a5-bbb1-468b-94cf-18622a542743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mid Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72c31e9c-2106-4716-a753-5c6806972d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "audio_indice = 1\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_50m50f_audio_model/'\n",
    "model_audio = model_alexnet()\n",
    "model_audio.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_audio.load_weights(save_dir +\"model_\"+str(audio_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b35c4f3-50b1-4736-a7f5-0473d6c49744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 48, 48, 1)\n",
      "Building Bilinear Interpolation Layer with input shape: ((None, 48, 48, 1), (None, 2, 3))\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.abspath(DIR)+'/cremad_50m50f_video_model/'\n",
    "video_indice = 1\n",
    "model_video = create__LRCN_with_STN()\n",
    "model_video.compile(loss='sparese_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model_video.load_weights(save_dir+\"model_\"+str(video_indice)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7512863-f622-47c1-83eb-f0b932890291",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_video_model=Model(inputs=model_video.input, outputs=model_video.get_layer('time_distributed_14').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46dfa709-c7a2-4fab-9803-e032b4c1b3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 21s 142ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video=features_video_model.predict(X_train_50m_50f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e818a4b0-e023-4eb6-92c1-efbf2116cb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 20, 1024)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9caa19ef-86cb-4954-9744-8ba2a165dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = np.reshape(features_video, (4678, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f6ed3b7-4f3c-4142-9ea7-e4deca0c5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 7s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "features_video_test = features_video_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10084b05-a3e0-4da0-945a-240f7c8684c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flattened = np.reshape(features_video_test, (1487, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95012120-ecfb-42b1-aa44-6e68cbceb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_audio_model=Model(inputs=model_audio.input, outputs=model_audio.get_layer('flatten_11').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b598e90-0f70-4511-b009-a511c331d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 44s 298ms/step\n"
     ]
    }
   ],
   "source": [
    "features_audio=features_audio_model.predict(X_train_aud_50m_50f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "147fe2e7-7723-4162-bb79-78f6f175d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 14s 289ms/step\n"
     ]
    }
   ],
   "source": [
    "features_audio_test = features_audio_model.predict(X_test_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac42dfe9-d730-4818-9acf-35be06274e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "video_scaler = StandardScaler().fit(X_train_flattened)\n",
    "video_data_standardized = video_scaler.fit_transform(X_train_flattened)\n",
    "video_data_standardized_test = video_scaler.fit_transform(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3fddcf19-4b31-45ed-acc6-0a7eb0c18598",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_scaler = StandardScaler().fit(features_audio)\n",
    "audio_data_standardized = audio_scaler.fit_transform(features_audio)\n",
    "audio_data_standardized_test = audio_scaler.fit_transform(features_audio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74feed4b-9844-4fb0-bc0a-db7225f46c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_video = PCA(n_components=0.95).fit(video_data_standardized)\n",
    "video_data_pca = pca_video.fit_transform(video_data_standardized)\n",
    "video_data_pca_test = pca_video.transform(video_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22a63b8e-0c9e-4f0b-a88e-a13ba687ebd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 1950)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d359d6f-7da4-4004-af3f-c0f9308a51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_audio = PCA(n_components=0.95).fit(audio_data_standardized)\n",
    "audio_data_pca = pca_audio.fit_transform(audio_data_standardized)\n",
    "audio_data_pca_test = pca_audio.transform(audio_data_standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "edf4ad76-d229-4fc5-ae6a-16a8574771fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 1846)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a8e010a-340e-4469-9bfe-e5ec50a43050",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_train_data = np.concatenate((video_data_pca, audio_data_pca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b297a5f-6ff8-46e8-96b0-1bf6553c2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_data = np.concatenate((video_data_pca_test, audio_data_pca_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "347023e2-421b-44fe-add0-baa03d70ebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4678, 3796)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a160e13-08a8-4542-ad59-094cbe392e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_NN():\n",
    "  model=Sequential()\n",
    "  model.add(Input(shape=(3796)))\n",
    "  \n",
    "  model.add(Dense(2096))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(1024))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(8))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2334e2e7-dd40-42c1-b7c8-c9e5bf703b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "936/936 [==============================] - 38s 39ms/step - loss: 1.5518 - accuracy: 0.4372 - val_loss: 1.0264 - val_accuracy: 0.6485\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 1.2011 - accuracy: 0.5508 - val_loss: 0.9465 - val_accuracy: 0.6677\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 1.0843 - accuracy: 0.5989 - val_loss: 0.9409 - val_accuracy: 0.6934\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.9959 - accuracy: 0.6368 - val_loss: 1.0241 - val_accuracy: 0.6464\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.9652 - accuracy: 0.6416 - val_loss: 0.9713 - val_accuracy: 0.6592\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.8799 - accuracy: 0.6852 - val_loss: 0.9721 - val_accuracy: 0.6560\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8262 - accuracy: 0.7028 - val_loss: 0.9917 - val_accuracy: 0.6485\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8101 - accuracy: 0.7047 - val_loss: 0.9775 - val_accuracy: 0.6635\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8225 - accuracy: 0.7138 - val_loss: 0.9745 - val_accuracy: 0.6720\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7896 - accuracy: 0.7181 - val_loss: 0.9641 - val_accuracy: 0.6731\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.7190 - accuracy: 0.7421 - val_loss: 1.0056 - val_accuracy: 0.6763\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7007 - accuracy: 0.7565 - val_loss: 1.0059 - val_accuracy: 0.6816\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.7172 - accuracy: 0.7491 - val_loss: 0.9685 - val_accuracy: 0.6827\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6391 - accuracy: 0.7811 - val_loss: 1.0376 - val_accuracy: 0.6795\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6724 - accuracy: 0.7731 - val_loss: 1.0342 - val_accuracy: 0.6677\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6628 - accuracy: 0.7777 - val_loss: 0.9918 - val_accuracy: 0.6624\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6357 - accuracy: 0.7769 - val_loss: 0.9850 - val_accuracy: 0.6827\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 34s 37ms/step - loss: 0.6131 - accuracy: 0.7897 - val_loss: 1.0228 - val_accuracy: 0.6613\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.9929 - accuracy: 0.6691\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 35s 37ms/step - loss: 1.5897 - accuracy: 0.4294 - val_loss: 0.9696 - val_accuracy: 0.6966\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 1.2121 - accuracy: 0.5612 - val_loss: 0.9045 - val_accuracy: 0.6880\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 34s 37ms/step - loss: 1.1022 - accuracy: 0.5909 - val_loss: 0.9285 - val_accuracy: 0.6774\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.9946 - accuracy: 0.6395 - val_loss: 0.8371 - val_accuracy: 0.7094\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.9444 - accuracy: 0.6547 - val_loss: 0.8629 - val_accuracy: 0.6848\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8725 - accuracy: 0.6780 - val_loss: 0.8717 - val_accuracy: 0.7147\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.8405 - accuracy: 0.7058 - val_loss: 0.8709 - val_accuracy: 0.6976\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.8387 - accuracy: 0.6996 - val_loss: 0.8583 - val_accuracy: 0.7030\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7913 - accuracy: 0.7309 - val_loss: 0.8846 - val_accuracy: 0.6955\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7565 - accuracy: 0.7352 - val_loss: 0.9037 - val_accuracy: 0.7019\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7172 - accuracy: 0.7427 - val_loss: 0.9069 - val_accuracy: 0.7083\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7086 - accuracy: 0.7576 - val_loss: 0.9188 - val_accuracy: 0.7030\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7088 - accuracy: 0.7491 - val_loss: 0.8912 - val_accuracy: 0.6923\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6351 - accuracy: 0.7873 - val_loss: 0.9389 - val_accuracy: 0.6934\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6697 - accuracy: 0.7630 - val_loss: 0.9436 - val_accuracy: 0.6827\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.6493 - accuracy: 0.7747 - val_loss: 0.9288 - val_accuracy: 0.6976\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6050 - accuracy: 0.7913 - val_loss: 0.9314 - val_accuracy: 0.6944\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6096 - accuracy: 0.7969 - val_loss: 0.9346 - val_accuracy: 0.7073\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5924 - accuracy: 0.7934 - val_loss: 0.9344 - val_accuracy: 0.7030\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6075 - accuracy: 0.7969 - val_loss: 0.9530 - val_accuracy: 0.7030\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.5911 - accuracy: 0.7964 - val_loss: 0.9781 - val_accuracy: 0.7041\n",
      "47/47 [==============================] - 1s 9ms/step - loss: 1.0053 - accuracy: 0.6732\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 35s 36ms/step - loss: 1.5856 - accuracy: 0.4297 - val_loss: 1.0408 - val_accuracy: 0.6464\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 1.1971 - accuracy: 0.5655 - val_loss: 0.9458 - val_accuracy: 0.6763\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 1.0898 - accuracy: 0.6015 - val_loss: 0.8903 - val_accuracy: 0.6880\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.9904 - accuracy: 0.6411 - val_loss: 0.9045 - val_accuracy: 0.6827\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.9393 - accuracy: 0.6553 - val_loss: 0.9037 - val_accuracy: 0.6923\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8703 - accuracy: 0.6801 - val_loss: 0.9278 - val_accuracy: 0.6891\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8556 - accuracy: 0.6841 - val_loss: 0.8857 - val_accuracy: 0.6859\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.8041 - accuracy: 0.7117 - val_loss: 0.9379 - val_accuracy: 0.6827\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7849 - accuracy: 0.7288 - val_loss: 0.9604 - val_accuracy: 0.6848\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.7489 - accuracy: 0.7392 - val_loss: 0.9277 - val_accuracy: 0.6774\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.7290 - accuracy: 0.7402 - val_loss: 0.9180 - val_accuracy: 0.7083\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6928 - accuracy: 0.7549 - val_loss: 0.9438 - val_accuracy: 0.6902\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6964 - accuracy: 0.7523 - val_loss: 0.9526 - val_accuracy: 0.6667\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6519 - accuracy: 0.7686 - val_loss: 0.9400 - val_accuracy: 0.6859\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6620 - accuracy: 0.7622 - val_loss: 1.0151 - val_accuracy: 0.6603\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.6277 - accuracy: 0.7894 - val_loss: 0.9994 - val_accuracy: 0.6816\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6133 - accuracy: 0.7905 - val_loss: 0.9866 - val_accuracy: 0.6912\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6144 - accuracy: 0.7900 - val_loss: 0.9497 - val_accuracy: 0.6838\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5962 - accuracy: 0.7961 - val_loss: 0.9577 - val_accuracy: 0.6955\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 32s 34ms/step - loss: 0.5842 - accuracy: 0.8049 - val_loss: 0.9060 - val_accuracy: 0.6912\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5933 - accuracy: 0.8028 - val_loss: 0.9512 - val_accuracy: 0.6763\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 32s 34ms/step - loss: 0.5607 - accuracy: 0.8084 - val_loss: 0.9954 - val_accuracy: 0.6880\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.5833 - accuracy: 0.7945 - val_loss: 0.9820 - val_accuracy: 0.6870\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.5315 - accuracy: 0.8164 - val_loss: 1.0216 - val_accuracy: 0.6806\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5585 - accuracy: 0.8014 - val_loss: 0.9589 - val_accuracy: 0.6902\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5309 - accuracy: 0.8204 - val_loss: 1.0396 - val_accuracy: 0.6752\n",
      "47/47 [==============================] - 1s 10ms/step - loss: 1.0361 - accuracy: 0.6725\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 35s 36ms/step - loss: 1.5514 - accuracy: 0.4435 - val_loss: 1.0098 - val_accuracy: 0.6460\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 1.2064 - accuracy: 0.5597 - val_loss: 0.9390 - val_accuracy: 0.6684\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 1.0830 - accuracy: 0.5985 - val_loss: 0.8689 - val_accuracy: 0.6995\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.9966 - accuracy: 0.6302 - val_loss: 0.8695 - val_accuracy: 0.6909\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.9394 - accuracy: 0.6594 - val_loss: 0.8726 - val_accuracy: 0.7059\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8996 - accuracy: 0.6698 - val_loss: 0.8632 - val_accuracy: 0.7144\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.8383 - accuracy: 0.7000 - val_loss: 0.8981 - val_accuracy: 0.7091\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.8366 - accuracy: 0.6954 - val_loss: 0.8301 - val_accuracy: 0.7112\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 34s 37ms/step - loss: 0.7730 - accuracy: 0.7342 - val_loss: 0.8304 - val_accuracy: 0.7187\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 34s 37ms/step - loss: 0.7624 - accuracy: 0.7299 - val_loss: 0.8668 - val_accuracy: 0.6941\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.7344 - accuracy: 0.7350 - val_loss: 0.9105 - val_accuracy: 0.6802\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7143 - accuracy: 0.7499 - val_loss: 0.8657 - val_accuracy: 0.7005\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6912 - accuracy: 0.7563 - val_loss: 0.8514 - val_accuracy: 0.7102\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.6797 - accuracy: 0.7662 - val_loss: 0.8550 - val_accuracy: 0.7016\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6718 - accuracy: 0.7646 - val_loss: 0.8719 - val_accuracy: 0.7059\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6435 - accuracy: 0.7801 - val_loss: 0.8527 - val_accuracy: 0.7219\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6036 - accuracy: 0.8026 - val_loss: 0.8860 - val_accuracy: 0.7080\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6277 - accuracy: 0.7793 - val_loss: 0.8328 - val_accuracy: 0.7080\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.6325 - accuracy: 0.7839 - val_loss: 0.8575 - val_accuracy: 0.7091\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5997 - accuracy: 0.7956 - val_loss: 0.8742 - val_accuracy: 0.7102\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5896 - accuracy: 0.7964 - val_loss: 0.8947 - val_accuracy: 0.7112\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5633 - accuracy: 0.8106 - val_loss: 0.9342 - val_accuracy: 0.6930\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.5609 - accuracy: 0.8090 - val_loss: 0.8941 - val_accuracy: 0.7134\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5436 - accuracy: 0.8149 - val_loss: 0.9307 - val_accuracy: 0.6952\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5613 - accuracy: 0.8074 - val_loss: 0.9293 - val_accuracy: 0.6898\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.5376 - accuracy: 0.8221 - val_loss: 0.8868 - val_accuracy: 0.7037\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5446 - accuracy: 0.8146 - val_loss: 0.9416 - val_accuracy: 0.6834\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5342 - accuracy: 0.8205 - val_loss: 0.9563 - val_accuracy: 0.6695\n",
      "Epoch 29/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5192 - accuracy: 0.8189 - val_loss: 0.9339 - val_accuracy: 0.6941\n",
      "Epoch 30/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5108 - accuracy: 0.8301 - val_loss: 0.9226 - val_accuracy: 0.6920\n",
      "Epoch 31/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5215 - accuracy: 0.8261 - val_loss: 0.9489 - val_accuracy: 0.6973\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.9557 - accuracy: 0.6785\n",
      "Epoch 1/80\n",
      "936/936 [==============================] - 35s 36ms/step - loss: 1.5432 - accuracy: 0.4438 - val_loss: 0.9848 - val_accuracy: 0.6749\n",
      "Epoch 2/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 1.2060 - accuracy: 0.5685 - val_loss: 0.9323 - val_accuracy: 0.6866\n",
      "Epoch 3/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 1.0870 - accuracy: 0.6003 - val_loss: 0.9081 - val_accuracy: 0.6813\n",
      "Epoch 4/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 1.0039 - accuracy: 0.6182 - val_loss: 0.8967 - val_accuracy: 0.6845\n",
      "Epoch 5/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.9524 - accuracy: 0.6505 - val_loss: 0.8746 - val_accuracy: 0.6930\n",
      "Epoch 6/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.9118 - accuracy: 0.6647 - val_loss: 0.8879 - val_accuracy: 0.6941\n",
      "Epoch 7/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.8197 - accuracy: 0.7096 - val_loss: 0.9046 - val_accuracy: 0.6824\n",
      "Epoch 8/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.8216 - accuracy: 0.7026 - val_loss: 0.8708 - val_accuracy: 0.6866\n",
      "Epoch 9/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.7641 - accuracy: 0.7382 - val_loss: 0.8656 - val_accuracy: 0.7016\n",
      "Epoch 10/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7478 - accuracy: 0.7315 - val_loss: 0.8896 - val_accuracy: 0.6727\n",
      "Epoch 11/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7349 - accuracy: 0.7417 - val_loss: 0.9181 - val_accuracy: 0.6920\n",
      "Epoch 12/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.7215 - accuracy: 0.7505 - val_loss: 0.8734 - val_accuracy: 0.7123\n",
      "Epoch 13/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.6934 - accuracy: 0.7582 - val_loss: 0.8681 - val_accuracy: 0.7027\n",
      "Epoch 14/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6735 - accuracy: 0.7724 - val_loss: 0.8843 - val_accuracy: 0.6973\n",
      "Epoch 15/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6606 - accuracy: 0.7828 - val_loss: 0.8775 - val_accuracy: 0.7091\n",
      "Epoch 16/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6686 - accuracy: 0.7662 - val_loss: 0.8653 - val_accuracy: 0.7059\n",
      "Epoch 17/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6431 - accuracy: 0.7836 - val_loss: 0.8702 - val_accuracy: 0.7037\n",
      "Epoch 18/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.6201 - accuracy: 0.7849 - val_loss: 0.8523 - val_accuracy: 0.7123\n",
      "Epoch 19/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5972 - accuracy: 0.7970 - val_loss: 0.8887 - val_accuracy: 0.6963\n",
      "Epoch 20/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5788 - accuracy: 0.7972 - val_loss: 0.8801 - val_accuracy: 0.7027\n",
      "Epoch 21/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5859 - accuracy: 0.7967 - val_loss: 0.8984 - val_accuracy: 0.7016\n",
      "Epoch 22/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.5762 - accuracy: 0.7994 - val_loss: 0.8549 - val_accuracy: 0.7187\n",
      "Epoch 23/80\n",
      "936/936 [==============================] - 34s 36ms/step - loss: 0.5573 - accuracy: 0.8146 - val_loss: 0.9341 - val_accuracy: 0.6952\n",
      "Epoch 24/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5624 - accuracy: 0.8034 - val_loss: 0.9216 - val_accuracy: 0.6930\n",
      "Epoch 25/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.5532 - accuracy: 0.8090 - val_loss: 0.9298 - val_accuracy: 0.7155\n",
      "Epoch 26/80\n",
      "936/936 [==============================] - 33s 36ms/step - loss: 0.5457 - accuracy: 0.8159 - val_loss: 0.9197 - val_accuracy: 0.7155\n",
      "Epoch 27/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5256 - accuracy: 0.8229 - val_loss: 0.9298 - val_accuracy: 0.6930\n",
      "Epoch 28/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5658 - accuracy: 0.8082 - val_loss: 0.8978 - val_accuracy: 0.7134\n",
      "Epoch 29/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5072 - accuracy: 0.8301 - val_loss: 0.9267 - val_accuracy: 0.7027\n",
      "Epoch 30/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5192 - accuracy: 0.8250 - val_loss: 0.9350 - val_accuracy: 0.7027\n",
      "Epoch 31/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5450 - accuracy: 0.8133 - val_loss: 0.9021 - val_accuracy: 0.7112\n",
      "Epoch 32/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5207 - accuracy: 0.8207 - val_loss: 0.9272 - val_accuracy: 0.6973\n",
      "Epoch 33/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5056 - accuracy: 0.8245 - val_loss: 0.9409 - val_accuracy: 0.7059\n",
      "Epoch 34/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.5064 - accuracy: 0.8373 - val_loss: 0.9373 - val_accuracy: 0.6952\n",
      "Epoch 35/80\n",
      "936/936 [==============================] - 32s 35ms/step - loss: 0.4943 - accuracy: 0.8336 - val_loss: 0.9469 - val_accuracy: 0.7027\n",
      "Epoch 36/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.4806 - accuracy: 0.8357 - val_loss: 0.9419 - val_accuracy: 0.6952\n",
      "Epoch 37/80\n",
      "936/936 [==============================] - 33s 35ms/step - loss: 0.4701 - accuracy: 0.8421 - val_loss: 0.9706 - val_accuracy: 0.6941\n",
      "47/47 [==============================] - 1s 10ms/step - loss: 1.0383 - accuracy: 0.6826\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_ACCURACY_SPEECH = []\n",
    "VALIDATION_LOSS_SPEECH = []\n",
    "\n",
    "save_dir = os.path.abspath(DIR) + '/cremad_40m60f_intermediatefusion_PCA_n0.95_model/'\n",
    "fold_var = 1\n",
    "\n",
    "for train_idx, val_idx in kfold.split(concatenated_train_data, y_train_40m_60f):\n",
    "    model=multi_modal_NN()\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_dir + get_model_name(fold_var), monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "# Start training the model.\n",
    "    LRCN_model_training_history = model.fit(      x = concatenated_train_data[train_idx],\n",
    "                                                  y = y_train_40m_60f[train_idx],\n",
    "                                                  validation_data=(concatenated_train_data[val_idx], y_train_40m_60f[val_idx]),\n",
    "                                                  epochs = 80,\n",
    "                                                  batch_size = 4,\n",
    "                                                  shuffle = True,\n",
    "                                                  callbacks = [checkpoint_cb, early_stopping_callback])\n",
    "    model.load_weights(save_dir + \"model_\" + str(fold_var) + \".h5\")\n",
    "\t\n",
    "    results = model.evaluate(concatenated_test_data, y_test)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "\t\n",
    "    VALIDATION_ACCURACY_SPEECH.append(results['accuracy'])\n",
    "    VALIDATION_LOSS_SPEECH.append(results['loss'])\n",
    "\t\n",
    "    tf.keras.backend.clear_session()\n",
    "\t\n",
    "    fold_var += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd241c3b-9fa9-4448-9c48-ad9c6b906988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 13ms/step - loss: 0.9729 - accuracy: 0.6698\n",
      "Test set evaluation: {'loss': 0.9728664755821228, 'accuracy': 0.6698049902915955}\n",
      "22/22 [==============================] - 0s 13ms/step\n",
      "25/25 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#Fairness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_50m50f_intermediatefusion_PCA_n0.95_model/'\n",
    "final_model = multi_modal_NN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_3.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(concatenated_test_data, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = concatenated_test_data[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "# for gender in np.unique(gender_test):\n",
    "#     gender_indices = np.where(gender_test == gender)\n",
    "#     print(gender)\n",
    "#     predicted_label_1 = predicted_labels[gender_indices]\n",
    "#     total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f44f075-0846-402d-9107-b4a3062b6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/CREMA-D_code/FairnessResults/fairness_analysis_cremad_50m50f_intermediatefusion_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163d42b-69b5-4c6c-8217-bb461e98cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unimodal Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "670042b0-794a-4d5b-a83f-f6c465e57e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Localization Network with input shape: (None, 48, 48, 1)\n",
      "Building Bilinear Interpolation Layer with input shape: ((None, 48, 48, 1), (None, 2, 3))\n",
      "47/47 [==============================] - 9s 151ms/step - loss: 1.6351 - accuracy: 0.5898\n",
      "Test set evaluation: {'loss': 1.635090708732605, 'accuracy': 0.5897780656814575}\n",
      "22/22 [==============================] - 4s 150ms/step\n",
      "25/25 [==============================] - 4s 147ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "save_dir = os.path.abspath(DIR)+'/cremad_50m50f_video_model/'\n",
    "final_model = create__LRCN_with_STN()\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "final_model.load_weights(save_dir + \"model_1.h5\")  # Adjust to load the best model if necessary\n",
    "\n",
    "test_results = final_model.evaluate(X_test, y_test)\n",
    "test_results = dict(zip(final_model.metrics_names, test_results))\n",
    "\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# Gender bias analysis\n",
    "metrics_by_gender = {}\n",
    "for gender in np.unique(gender_test):\n",
    "    gender_indices = np.where(gender_test == gender)\n",
    "    X_gender_test = X_test[gender_indices]\n",
    "    y_gender_test = y_test[gender_indices]\n",
    "    y_pred = np.argmax(final_model.predict(X_gender_test), axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_gender_test)\n",
    "    precision = precision_score(y_gender_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_gender_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_gender_test, y_pred, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(y_gender_test, y_pred)\n",
    "    tn = np.sum(cm) - (np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    fp = np.sum(cm, axis=0) - np.diag(cm)\n",
    "    fn = np.sum(cm, axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    precision_value = tp / (tp + fp)\n",
    "\n",
    "    demographic_parity_dict = {}\n",
    "    total_count  = len(y_pred)\n",
    "    for emotion_idx in np.unique(y_pred):\n",
    "        #emotion_idx = int(emotion_idx)\n",
    "        count_emotion = np.count_nonzero(y_pred == emotion_idx)\n",
    "        demographic_parity = count_emotion / total_count \n",
    "        emotion_name = emotions_en[emotion_idx]  # Get the emotion name from the index\n",
    "        demographic_parity_dict[emotion_name] = demographic_parity\n",
    "\n",
    "    \n",
    "    metrics_by_gender[gender] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tpr': tpr,\n",
    "        'fpr_value': fpr,\n",
    "        'fnr_value': fnr,\n",
    "        'precision_value': precision_value,\n",
    "        'demographic_parity': demographic_parity_dict\n",
    "    }\n",
    "    \n",
    "\n",
    "#Additional fairness metrics\n",
    "\n",
    "# for gender in np.unique(gender_test):\n",
    "#     gender_indices = np.where(gender_test == gender)\n",
    "#     print(gender)\n",
    "#     predicted_label_1 = predicted_labels[gender_indices]\n",
    "#     total_count = len(predicted_label_1)\n",
    "    \n",
    "\n",
    "for gender in metrics_by_gender:\n",
    "    #metrics_by_gender[gender]['demographic_parity'] = metrics_by_gender[gender]['positive_rate']\n",
    "    metrics_by_gender[gender]['equalized_odds'] = (metrics_by_gender[gender]['tpr'], metrics_by_gender[gender]['fpr_value'])\n",
    "    metrics_by_gender[gender]['equal_opportunity'] = metrics_by_gender[gender]['tpr']\n",
    "    metrics_by_gender[gender]['predictive_parity'] = metrics_by_gender[gender]['precision_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cd95579-e4e2-4ef0-854e-98ef4508dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['gender', 'accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr_value', 'fnr_value', 'precision_value', 'demographic_parity']\n",
    "import csv\n",
    "file_path = '/var/scratch/mpa326/CREMA-D_code/FairnessResults/fairness_analysis_cremad_50m50f_video_model.csv'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "# Write the dictionary to a CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for gender, metrics in metrics_by_gender.items():\n",
    "        row = [gender]\n",
    "        for key in header[1:]:  # Exclude 'gender' from header keys\n",
    "            value = metrics.get(key, None)\n",
    "            # Convert dictionaries to string for CSV export\n",
    "            if isinstance(value, dict):\n",
    "                value = str(value)\n",
    "            row.append(value)\n",
    "        writer.writerow(row)\n",
    "    overall_row = ['overall']\n",
    "    for key in header[1:]:\n",
    "        # Add overall accuracy and loss, leave other columns empty (None)\n",
    "        if key in test_results:\n",
    "            overall_row.append(test_results[key])\n",
    "        else:\n",
    "            overall_row.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cc1a2-2d59-499a-a3f0-dfba071bf6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
